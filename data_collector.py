#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Mark Minervini Screener - ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ

import os
import csv
import time
import requests
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
from pytz import timezone
from concurrent.futures import ThreadPoolExecutor
from typing import List, Set

from utils import (
    ensure_dir, get_us_market_today, clean_tickers, safe_filename
)

from config import (
    DATA_DIR, DATA_US_DIR, RESULTS_DIR
)

# ì£¼ì‹ ì‹¬ë³¼ ìˆ˜ì§‘ (NASDAQ API ì œê±°ë¨ - íƒ€ì„ì•„ì›ƒ ë¬¸ì œë¡œ ì¸í•´)
# ëŒ€ì‹  Yahoo Financeë‚˜ ë‹¤ë¥¸ ì•ˆì •ì ì¸ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

def get_sp500_symbols() -> List[str]:
    """S&P 500 êµ¬ì„± ì¢…ëª© ê°€ì ¸ì˜¤ê¸°"""
    try:
        print("ğŸ“ˆ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì¤‘...")
        sp500_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        tables = pd.read_html(sp500_url)
        sp500_df = tables[0]
        symbols = sp500_df['Symbol'].tolist()
        
        # ì¼ë¶€ ê¸°í˜¸ ì •ë¦¬ (ì˜ˆ: BRK.B -> BRK-B)
        cleaned_symbols = []
        for symbol in symbols:
            if '.' in symbol:
                symbol = symbol.replace('.', '-')
            cleaned_symbols.append(symbol)
        
        print(f"âœ… S&P 500 ì¢…ëª© {len(cleaned_symbols)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return cleaned_symbols
    except Exception as e:
        print(f"âš ï¸ S&P 500 ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def get_nasdaq_100_symbols() -> List[str]:
    """NASDAQ 100 êµ¬ì„± ì¢…ëª© ê°€ì ¸ì˜¤ê¸°"""
    try:
        print("ğŸ“ˆ NASDAQ 100 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì¤‘...")
        nasdaq_url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        tables = pd.read_html(nasdaq_url)
        nasdaq_df = tables[4]  # NASDAQ 100 êµ¬ì„± ì¢…ëª© í…Œì´ë¸”
        symbols = nasdaq_df['Ticker'].tolist()
        
        print(f"âœ… NASDAQ 100 ì¢…ëª© {len(symbols)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return symbols
    except Exception as e:
        print(f"âš ï¸ NASDAQ 100 ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def get_ipo_symbols() -> List[str]:
    """ìµœê·¼ IPO ì¢…ëª©ì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ"""
    try:
        print("ğŸ¢ ìµœê·¼ IPO ì¢…ëª© í™•ì¸ ì¤‘...")
        from screeners.ipo_investment.ipo_data_collector import RealIPODataCollector
        
        collector = RealIPODataCollector()
        result = collector.collect_all_ipo_data()
        
        symbols = []
        # ìµœê·¼ IPOì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ
        for ipo in result.get('recent_ipos', []):
            symbol = ipo.get('symbol', ipo.get('ticker', ''))
            if symbol and symbol.strip():
                symbols.append(symbol.strip())
        
        # ì˜ˆì •ëœ IPOì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ
        for ipo in result.get('upcoming_ipos', []):
            symbol = ipo.get('symbol', ipo.get('ticker', ''))
            if symbol and symbol.strip():
                symbols.append(symbol.strip())
        
        # ì¤‘ë³µ ì œê±°
        symbols = list(set(symbols))
        print(f"âœ… IPO ì¢…ëª© {len(symbols)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return symbols
    except Exception as e:
        print(f"âš ï¸ IPO ì¢…ëª© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def update_symbol_list() -> Set[str]:
    """ìƒˆë¡œìš´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
    print("\nğŸ”„ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    
    # ê¸°ì¡´ ì¢…ëª© ëª©ë¡
    try:
        from data_collectors.stock_metadata_collector import get_symbols
        existing_symbols = set(get_symbols())
        print(f"ğŸ“Š ê¸°ì¡´ ì¢…ëª© ìˆ˜: {len(existing_symbols)}ê°œ")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì¢…ëª© ë¡œë“œ ì‹¤íŒ¨: {e}")
        existing_symbols = set()
    
    # ìƒˆë¡œìš´ ì¢…ëª© ìˆ˜ì§‘
    new_symbols = set()
    
    # S&P 500 ì¢…ëª© ì¶”ê°€
    sp500_symbols = get_sp500_symbols()
    new_symbols.update(sp500_symbols)
    
    # NASDAQ 100 ì¢…ëª© ì¶”ê°€
    nasdaq_symbols = get_nasdaq_100_symbols()
    new_symbols.update(nasdaq_symbols)
    
    # IPO ì¢…ëª© ì¶”ê°€
    ipo_symbols = get_ipo_symbols()
    new_symbols.update(ipo_symbols)
    
    # ê¸°ì¡´ì— ì—†ëŠ” ìƒˆë¡œìš´ ì¢…ëª©ë§Œ í•„í„°ë§
    truly_new_symbols = new_symbols - existing_symbols
    
    if truly_new_symbols:
        print(f"ğŸ†• ìƒˆë¡œ ë°œê²¬ëœ ì¢…ëª©: {len(truly_new_symbols)}ê°œ")
        print(f"   ì˜ˆì‹œ: {list(truly_new_symbols)[:10]}")
        
        # ìƒˆë¡œìš´ ì¢…ëª©ë“¤ì˜ ë¹ˆ CSV íŒŒì¼ ìƒì„± (ë‹¤ìŒ ìˆ˜ì§‘ ì‹œ í¬í•¨ë˜ë„ë¡)
        for symbol in truly_new_symbols:
            try:
                safe_symbol = safe_filename(symbol)
                csv_path = os.path.join(DATA_US_DIR, f"{safe_symbol}.csv")
                if not os.path.exists(csv_path):
                    # ë¹ˆ CSV íŒŒì¼ ìƒì„± (í—¤ë”ë§Œ)
                    empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                    empty_df.to_csv(csv_path, index=False)
                    print(f"ğŸ“ ìƒˆ ì¢…ëª© íŒŒì¼ ìƒì„±: {symbol}")
            except Exception as e:
                print(f"âš ï¸ {symbol} íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
    else:
        print("âœ… ìƒˆë¡œìš´ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ì¢…ëª© ëª©ë¡ ë°˜í™˜
    all_symbols = existing_symbols.union(new_symbols)
    print(f"ğŸ“ˆ ì´ ì¢…ëª© ìˆ˜: {len(all_symbols)}ê°œ")
    return all_symbols

# ë¯¸êµ­ ì£¼ì‹ ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def fetch_us_single(ticker, start, end):
    """yfinance APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼
        start: ì‹œì‘ ë‚ ì§œ
        end: ì¢…ë£Œ ë‚ ì§œ
        
    Returns:
        DataFrame: ì£¼ê°€ ë°ì´í„° ë˜ëŠ” None(ì˜¤ë¥˜ ë°œìƒ ì‹œ)
    """
    try:
        # ìš”ì²­ ì „ ì§§ì€ ëŒ€ê¸° ì¶”ê°€ (API ì œí•œ ë°©ì§€)
        time.sleep(0.5)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
        print(f"[US] ğŸ“Š ë°ì´í„° ìš”ì²­ ì¤‘: {ticker} ({start} ~ {end})")
        ticker_obj = yf.Ticker(ticker)
        
        # income_stmtì—ì„œ Net Income ê°€ì ¸ì˜¤ê¸°
        try:
            income_stmt = ticker_obj.income_stmt
            if income_stmt is not None and not income_stmt.empty:
                net_income = income_stmt.loc['Net Income'] if 'Net Income' in income_stmt.index else None
                if net_income is not None:
                    print(f"[US] âœ… Net Income ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker}")
        except Exception as e:
            print(f"[US] âš ï¸ Net Income ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {str(e)}")
        
        # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        df = ticker_obj.history(start=start, end=end, interval="1d",
                               auto_adjust=False, actions=False, timeout=10)
        
        if df.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„° ë°˜í™˜ë¨: {ticker}")
            # ë¹ˆ ë°ì´í„°ë„ ìƒì¥ íì§€ ì¢…ëª©ìœ¼ë¡œ ì²˜ë¦¬ (ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
            
        print(f"[US] âœ… ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker} ({len(df)} í–‰)")
        df = df.rename_axis("date").reset_index()
        df["symbol"] = ticker
        return df
    except requests.exceptions.ReadTimeout:
        print(f"[US] â±ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {ticker} - API ì‘ë‹µ ì§€ì—° (ì¬ì‹œë„ í•„ìš”)")
        return None
    except requests.exceptions.ConnectionError:
        print(f"[US] ğŸŒ ì—°ê²° ì˜¤ë¥˜: {ticker} - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë°œìƒ (ì¬ì‹œë„ í•„ìš”)")
        return None
    except Exception as e:
        error_msg = str(e).lower()
        # ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ ë° ì²˜ë¦¬ (ê°ì§€ ì¡°ê±´ í™•ì¥)
        delisted_keywords = ["delisted", "no timezone found", "possibly delisted", "not found", 
                            "invalid ticker", "symbol may be delisted", "404"]
        if any(keyword in error_msg for keyword in delisted_keywords):
            print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
            # ë¹ˆ íŒŒì¼ ìƒì„±í•˜ì—¬ ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•Šë„ë¡ í•¨
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
        elif "rate limit" in error_msg or "429" in error_msg:
            print(f"[US] ğŸš« API ì œí•œ ë„ë‹¬: {ticker} - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
        else:
            print(f"[US] âŒ ì˜¤ë¥˜ ë°œìƒ ({ticker}): {error_msg[:100]}")
        return None

# ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „)
def fetch_and_save_us_ohlcv_chunked(tickers, save_dir=DATA_US_DIR, chunk_size=5, pause=5.0, start_chunk=0, max_chunks=None, max_workers=3):
    ensure_dir(save_dir)
    today = get_us_market_today()
    
    # ì§„í–‰ ìƒí™© ì €ì¥ íŒŒì¼
    progress_file = os.path.join(DATA_DIR, "fetch_progress.txt")
    
    # ì´ ì²­í¬ ìˆ˜ ê³„ì‚°
    total_chunks = (len(tickers) + chunk_size - 1) // chunk_size
    if max_chunks is not None:
        total_chunks = min(total_chunks, start_chunk + max_chunks)

    # ì‹œì‘ ì²­í¬ ì„¤ì •
    if start_chunk > 0:
        print(f"ğŸ”„ ì²­í¬ {start_chunk}ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤ (ì´ {total_chunks} ì²­í¬)")
    
    # í‹°ì»¤ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    def process_ticker(ticker):
        # Windows ì˜ˆì•½ íŒŒì¼ëª… ì²˜ë¦¬
        safe_ticker = safe_filename(ticker)
        path = os.path.join(save_dir, f"{safe_ticker}.csv")
        if os.path.exists(path):
            try:
                existing = pd.read_csv(path)
                existing["date"] = pd.to_datetime(existing["date"], utc=True)
                if "date" not in existing.columns:
                    raise ValueError("âŒ 'date' ì»¬ëŸ¼ ì—†ìŒ")

                # ë‚ ì§œ ë°ì´í„°ë¥¼ UTCë¡œ ë³€í™˜
                existing["date"] = pd.to_datetime(existing["date"], utc=True)

                if len(existing) == 0:
                    # í—¤ë”ë§Œ ì¡´ì¬í•˜ëŠ” ë¹ˆ íŒŒì¼ì˜ ê²½ìš° ìƒˆë¡œ ìˆ˜ì§‘
                    print(f"[US] ğŸ“Š ë¹ˆ íŒŒì¼ ê°ì§€, ìƒˆë¡œ ë°ì´í„° ìˆ˜ì§‘: {ticker}")
                    existing = None
                    start_date = today - timedelta(days=450)
                else:
                    # ë‚ ì§œ ì»¬ëŸ¼ì´ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                        existing["date"] = pd.to_datetime(existing["date"], utc=True)

                    # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„°ê°€ 330ì¼ ì´ìƒì¸ ê²½ìš° ì˜¤ë˜ëœ ë°ì´í„° ì œê±°)
                    if len(existing) > 330:
                        print(f"[US] âœ‚ï¸ {ticker}: 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(existing)} â†’ 330)")
                        existing = existing.sort_values("date", ascending=False).head(330).reset_index(drop=True)
                        # ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ë‹¤ì‹œ ì •ë ¬
                        existing = existing.sort_values("date", ascending=True).reset_index(drop=True)

                    last_date = existing["date"].dropna().max().date()
                    start_date = last_date + timedelta(days=1)
            except Exception as e:
                print(f"âš ï¸ {ticker} ê¸°ì¡´ íŒŒì¼ ì˜¤ë¥˜: {e}")
                existing = None
                start_date = today - timedelta(days=450)
        else:
            existing = None
            start_date = today - timedelta(days=450)

        if start_date >= today:
            print(f"[US] â© ìµœì‹  ìƒíƒœ (ë˜ëŠ” ì˜¤ëŠ˜ì€ ê±°ë˜ì¼ ì•„ë‹˜): {ticker}")
            return False

        print(f"[DEBUG] {ticker}: ìˆ˜ì§‘ ì‹œì‘ì¼ {start_date}, ì¢…ë£Œì¼ {today}")

        df_new = None
        for j in range(5):  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            try:
                df_new = fetch_us_single(ticker, start=start_date, end=today)
                
                # ìƒì¥ íì§€ ì¢…ëª© ì²˜ë¦¬ (ë¹ˆ DataFrameì´ ë°˜í™˜ëœ ê²½ìš°)
                if df_new is not None and df_new.empty:
                    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìƒì¥íì§€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                    if existing is not None and len(existing) > 0:
                        print(f"[US] â© ìƒˆ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {ticker}")
                        return False
                    else:
                        print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                        # ë¹ˆ DataFrameì— ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì €ì¥, ì—†ìœ¼ë©´ ê¸°ë³¸ ì»¬ëŸ¼ ì¶”ê°€
                        if len(df_new.columns) == 0:
                            df_new = pd.DataFrame(columns=['date', 'symbol', 'open', 'high', 'low', 'close', 'volume'])
                        df_new.to_csv(path, index=False)
                        return True
                elif df_new is not None and not df_new.empty:
                    # ì •ìƒ ë°ì´í„° íšë“
                    break
                    
                print(f"[US] âš ï¸ {ticker} ë¹ˆ ë°ì´í„° ë°˜í™˜, ì¬ì‹œë„ {j+1}/5")
            except Exception as e:
                error_msg = str(e).lower()
                # ìƒì¥ íì§€ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                delisted_keywords = ["delisted", "no timezone", "possibly delisted", "not found", "invalid ticker", "404"]
                if any(keyword in error_msg for keyword in delisted_keywords):
                    print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                    # ë¹ˆ íŒŒì¼ ìƒì„±
                    empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                    empty_df.to_csv(path, index=False)
                    return True
                    
                wait = 2 ** j + 2  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                print(f"[US] âš ï¸ {ticker} ì¬ì‹œë„ {j+1}/5 â†’ {wait}s ëŒ€ê¸°: {error_msg[:100]}")
                time.sleep(wait)
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¶”ê°€ ëŒ€ê¸°
            if j < 4:  # ë§ˆì§€ë§‰ ì‹œë„ ì „ê¹Œì§€
                time.sleep(1)  # API ìš”ì²­ ì‚¬ì´ì— ì¶”ê°€ ëŒ€ê¸°

        if df_new is None or df_new.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„°: {ticker}")
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìƒì¥íì§€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            if existing is not None and len(existing) > 0:
                print(f"[US] â© ìƒˆ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {ticker}")
                return False
            else:
                # ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ë„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒì¥ íì§€ë¡œ ê°„ì£¼
                empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                empty_df.to_csv(path, index=False)
                print(f"[US] ğŸš« ë°ì´í„° ì—†ìŒ - ìƒì¥ íì§€ë¡œ ì²˜ë¦¬: {ticker}")
                return True

        if existing is not None:
            before_len = len(existing)
            
            # ë‚ ì§œ í˜•ì‹ í†µì¼ (ëª¨ë“  ë‚ ì§œë¥¼ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜)
            if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                existing["date"] = pd.to_datetime(existing["date"], utc=True)
            if not pd.api.types.is_datetime64tz_dtype(df_new["date"]):
                df_new["date"] = pd.to_datetime(df_new["date"], utc=True)
                
            # ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±° (ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°)
            existing["date_str"] = existing["date"].dt.strftime("%Y-%m-%d")
            df_new["date_str"] = df_new["date"].dt.strftime("%Y-%m-%d")
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìƒˆ ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” ë‚ ì§œ ì œê±°
            existing_filtered = existing[~existing["date_str"].isin(df_new["date_str"])]
            
            # ë°ì´í„° ë³‘í•©
            df_combined = pd.concat([existing_filtered, df_new], ignore_index=True)
            
            # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
            df_combined.drop("date_str", axis=1, inplace=True)
            
            # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„° ë³‘í•© í›„ ë‹¤ì‹œ í™•ì¸)
            if len(df_combined) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ë³‘í•© í›„ 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(df_combined)} â†’ 330)")
                df_combined = df_combined.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_combined = df_combined.sort_values("date", ascending=True).reset_index(drop=True)
            
            after_len = len(df_combined)

            # í•­ìƒ ì €ì¥í•˜ì—¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ì¥
            df_combined.to_csv(path, index=False)
            if after_len > before_len:
                print(f"[US] âœ… ì €ì¥ë¨: {ticker} ({after_len} rows, +{after_len - before_len})")
            else:
                print(f"[US] ğŸ”„ ë°ì´í„° ì—…ë°ì´íŠ¸ë¨: {ticker} ({after_len} rows)")
            return True
        else:
            # ì‹ ê·œ ë°ì´í„°ë„ 330 ì˜ì—…ì¼ ì œí•œ ì ìš©
            if len(df_new) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ì‹ ê·œ ë°ì´í„° 330 ì˜ì—…ì¼ ì´ˆê³¼ ì •ë¦¬ ì¤‘ ({len(df_new)} â†’ 330)")
                df_new = df_new.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_new = df_new.sort_values("date", ascending=True).reset_index(drop=True)
                
            df_new.to_csv(path, index=False)
            print(f"[US] âœ… ì‹ ê·œ ì €ì¥: {ticker} ({len(df_new)} rows)")
            return True
    
    # ì²­í¬ë³„ ì²˜ë¦¬ ì‹œì‘
    for i in range(start_chunk * chunk_size, len(tickers), chunk_size):
        chunk_num = i // chunk_size
        
        # ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ í™•ì¸
        if max_chunks is not None and chunk_num >= start_chunk + max_chunks:
            print(f"ğŸ›‘ ìµœëŒ€ ì²­í¬ ìˆ˜ ({max_chunks}) ë„ë‹¬. ì‘ì—… ì¤‘ë‹¨.")
            break
            
        chunk = tickers[i:i+chunk_size]
        
        # NaN ê°’ í•„í„°ë§
        chunk = [t for t in chunk if isinstance(t, str) or (isinstance(t, (int, float)) and not pd.isna(t))]
        
        print(f"\nâ±ï¸ Chunk {chunk_num + 1}/{total_chunks} ì‹œì‘ ({len(chunk)}ê°œ): {chunk}")
        
        # ì§„í–‰ ìƒí™© ì €ì¥
        with open(progress_file, 'w') as f:
            f.write(str(chunk_num))
        
        # ì²­í¬ ë‚´ í‹°ì»¤ ë³‘ë ¬ ì²˜ë¦¬
        success_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë³‘ë ¬ë¡œ í‹°ì»¤ ì²˜ë¦¬ ì‹¤í–‰
            results = list(executor.map(process_ticker, chunk))
            success_count = sum(1 for result in results if result)
        
        # ì²­í¬ ì™„ë£Œ í›„ ìƒíƒœ ì¶œë ¥ ë° ëŒ€ê¸°
        print(f"âœ… ì²­í¬ {chunk_num + 1}/{total_chunks} ì™„ë£Œ: {success_count}/{len(chunk)} ì„±ê³µ")
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        if chunk_num + 1 < total_chunks:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
            print(f"â³ {pause}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(pause)

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

# ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
def collect_data(max_us_chunks=None, start_chunk=0, update_symbols=True):
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for directory in [DATA_DIR, DATA_US_DIR, RESULTS_DIR]:
        ensure_dir(directory)
        
    print("\nğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì„ íƒì )
    if update_symbols:
        try:
            print("\nğŸ”„ 1ë‹¨ê³„: ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸")
            all_symbols = update_symbol_list()
            us_tickers = list(all_symbols)
        except Exception as e:
            print(f"âš ï¸ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            print("ğŸ“Š ê¸°ì¡´ CSV íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            from data_collectors.stock_metadata_collector import get_symbols
            us_tickers = get_symbols()
    else:
        print("\nğŸ“Š ê¸°ì¡´ CSV íŒŒì¼ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì¢…ëª© ëª©ë¡ ìƒì„±")
        from data_collectors.stock_metadata_collector import get_symbols
        us_tickers = get_symbols()
    
    if not us_tickers:
        print("âš ï¸ ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“Š 2ë‹¨ê³„: ì´ {len(us_tickers)}ê°œ ì¢…ëª©ì˜ OHLCV ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
    
    # OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    try:
        fetch_and_save_us_ohlcv_chunked(
            tickers=us_tickers,
            save_dir=DATA_US_DIR,
            chunk_size=10,
            pause=2.0,
            start_chunk=start_chunk,
            max_chunks=max_us_chunks,
            max_workers=5
        )
        
    except Exception as e:
        print(f"âŒ OHLCV ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("âš ï¸ ë°ì´í„° ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

# ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Mark Minervini ìŠ¤í¬ë¦¬ë„ˆ - ë°ì´í„° ìˆ˜ì§‘")
    parser.add_argument("--max-us-chunks", type=int, help="ìµœëŒ€ ë¯¸êµ­ ì£¼ì‹ ì²­í¬ ìˆ˜ ì œí•œ")
    parser.add_argument("--start-chunk", type=int, default=0, help="ì‹œì‘í•  ì²­í¬ ë²ˆí˜¸")
    parser.add_argument("--no-symbol-update", action="store_true", help="ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    collect_data(
        max_us_chunks=args.max_us_chunks,
        start_chunk=args.start_chunk,
        update_symbols=not args.no_symbol_update
    )