#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Mark Minervini Screener - ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ

import os
import csv
import time
import requests
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
from pytz import timezone
from concurrent.futures import ThreadPoolExecutor

from utils import (
    ensure_dir, get_us_market_today, clean_tickers, safe_filename
)

from config import (
    DATA_DIR, DATA_US_DIR, RESULTS_DIR
)

# NASDAQ, NYSE, ETF í‹°ì»¤ ìˆ˜ì§‘
def load_nasdaq_ftp_symbols():
    base_url = "https://www.nasdaqtrader.com/dynamic/SymDir"

    files = {
        "nasdaq": {
            "file": "nasdaqlisted.txt",
            "symbol_col": "Symbol",
            "test_col": "Test Issue"
        },
        "nyse": {
            "file": "otherlisted.txt",
            "symbol_col": "ACT Symbol",
            "test_col": "Test Issue"
        },
        "etf": {
            "file": "etf.txt",
            "symbol_col": None,
            "test_col": None
        }
    }

    all_symbols = []

    for name, meta in files.items():
        url = f"{base_url}/{meta['file']}"
        try:
            res = requests.get(url)
            content = res.text.strip().splitlines()
            if "File Creation Time" in content[-1]:
                content = content[:-1]
            reader = csv.reader(content, delimiter="|")
            rows = list(reader)

            if name == "etf":
                df = pd.DataFrame(rows[1:], columns=["Symbol", "Name", "IsEnabled"])
                symbols = df["Symbol"].dropna().astype(str).tolist()
            else:
                header, *data = rows
                df = pd.DataFrame(data, columns=header)
                symbol_col = meta["symbol_col"]
                test_col = meta["test_col"]
                if test_col in df.columns:
                    df = df[df[test_col] != "Y"]
                symbols = df[symbol_col].dropna().astype(str).tolist()

            # HTML íƒœê·¸ ë° JavaScript/CSS ì½”ë“œê°€ í¬í•¨ëœ í‹°ì»¤ í•„í„°ë§
            filtered_symbols = []
            for symbol in symbols:
                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
                if not symbol or symbol.isspace():
                    continue
                    
                # ìœ íš¨í•œ í‹°ì»¤ ì‹¬ë³¼ íŒ¨í„´ ê²€ì‚¬ (ì•ŒíŒŒë²³, ìˆ«ì, ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìë§Œ í—ˆìš©)
                if not all(c.isalnum() or c in '.-$^' for c in symbol):
                    # ë¡œê·¸ ì‹œì‘ ë¶€ë¶„
                    log_msg = f"âš ï¸ ë¹„ì •ìƒì ì¸ í‹°ì»¤ ì œì™¸: {symbol}"
                    reasons = []
                    
                    # HTML íƒœê·¸ íŒ¨í„´ ê°ì§€
                    if '<' in symbol or '>' in symbol:
                        reasons.append("HTML íƒœê·¸ í¬í•¨")
                    
                    # CSS ìŠ¤íƒ€ì¼ ì½”ë“œ íŒ¨í„´ ê°ì§€
                    css_patterns = ['{', '}', ':', ';']
                    css_keywords = ['width', 'height', 'position', 'margin', 'padding', 'color', 'background', 'font', 'display', 'style']
                    
                    if any(p in symbol for p in css_patterns):
                        reasons.append("CSS êµ¬ë¬¸ í¬í•¨")
                    elif any(kw in symbol.lower() for kw in css_keywords):
                        reasons.append("CSS ì†ì„± í¬í•¨")
                    
                    # JavaScript ì½”ë“œ íŒ¨í„´ ê°ì§€
                    js_patterns = ['=', '(', ')', '[', ']', '&&', '||', '!', '?', '.']
                    js_keywords = ['function', 'var', 'let', 'const', 'return', 'if', 'else', 'for', 'while', 'class', 'new', 'this', 'document', 'window']
                    
                    if any(p in symbol for p in js_patterns):
                        reasons.append("JS êµ¬ë¬¸ í¬í•¨")
                    elif any(kw in symbol.lower() for kw in js_keywords):
                        reasons.append("JS í‚¤ì›Œë“œ í¬í•¨")
                    elif '.className' in symbol or 'RegExp' in symbol:
                        reasons.append("JS API í¬í•¨")
                    
                    # JavaScript ì£¼ì„ íŒ¨í„´ ê°ì§€
                    if symbol.strip().startswith('//') or symbol.strip().startswith('/*') or symbol.strip().endswith('*/'):
                        reasons.append("JS ì£¼ì„ í¬í•¨")
                    
                    # ê¸°íƒ€ ë¹„ì •ìƒ íŒ¨í„´
                    if not reasons:
                        reasons.append("ë¹„ì •ìƒ ë¬¸ì í¬í•¨")
                    
                    # ë¡œê·¸ ì¶œë ¥
                    print(f"{log_msg} - ì´ìœ : {', '.join(reasons)}")
                    continue
                
                # í‹°ì»¤ ê¸¸ì´ ì œí•œ (ì¼ë°˜ì ìœ¼ë¡œ 1-5ì)
                if len(symbol) > 8:
                    print(f"âš ï¸ ë„ˆë¬´ ê¸´ í‹°ì»¤ ì œì™¸ ({len(symbol)}ì): {symbol}")
                    continue
                    
                filtered_symbols.append(symbol)
            
            all_symbols.extend(filtered_symbols)
            print(f"âœ… {name.upper()} ì¢…ëª© ìˆ˜: {len(filtered_symbols)}")
        except Exception as e:
            print(f"âŒ {name.upper()} ë¡œë”© ì‹¤íŒ¨: {e}")

    unique_cleaned = clean_tickers(all_symbols)
    print(f"ğŸ¯ ìµœì¢… í´ë¦° í‹°ì»¤ ìˆ˜: {len(unique_cleaned)}")
    return unique_cleaned

# ìºì‹œëœ NASDAQ ì‹¬ë³¼ ê°€ì ¸ì˜¤ê¸°
def get_or_load_cached_nasdaq_symbols():
    nasdaq_cache_path = os.path.join(DATA_DIR, "nasdaq_symbols.csv")
    ensure_dir(os.path.dirname(nasdaq_cache_path))
    if os.path.exists(nasdaq_cache_path):
        # ìºì‹œ íŒŒì¼ì´ 24ì‹œê°„ ì´ìƒ ì§€ë‚¬ëŠ”ì§€ í™•ì¸
        file_time = datetime.fromtimestamp(os.path.getmtime(nasdaq_cache_path), tz=timezone('UTC'))
        if datetime.now(timezone('UTC')) - file_time < timedelta(hours=24):
            print("ğŸ“‚ NASDAQ ìºì‹œ ë¡œë“œ ì¤‘...")
            df = pd.read_csv(nasdaq_cache_path)
            return df['symbol'].tolist()
    
    print("ğŸŒ NASDAQ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì¤‘...")
    symbols = load_nasdaq_ftp_symbols()
    pd.DataFrame({'symbol': symbols}).to_csv(nasdaq_cache_path, index=False)
    return symbols

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

# ë¯¸êµ­ ì£¼ì‹ ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def fetch_us_single(ticker, start, end):
    """yfinance APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼
        start: ì‹œì‘ ë‚ ì§œ
        end: ì¢…ë£Œ ë‚ ì§œ
        
    Returns:
        DataFrame: ì£¼ê°€ ë°ì´í„° ë˜ëŠ” None(ì˜¤ë¥˜ ë°œìƒ ì‹œ)
    """
    try:
        # ìš”ì²­ ì „ ì§§ì€ ëŒ€ê¸° ì¶”ê°€ (API ì œí•œ ë°©ì§€)
        time.sleep(0.5)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
        print(f"[US] ğŸ“Š ë°ì´í„° ìš”ì²­ ì¤‘: {ticker} ({start} ~ {end})")
        ticker_obj = yf.Ticker(ticker)
        
        # income_stmtì—ì„œ Net Income ê°€ì ¸ì˜¤ê¸°
        try:
            income_stmt = ticker_obj.income_stmt
            if income_stmt is not None and not income_stmt.empty:
                net_income = income_stmt.loc['Net Income'] if 'Net Income' in income_stmt.index else None
                if net_income is not None:
                    print(f"[US] âœ… Net Income ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker}")
        except Exception as e:
            print(f"[US] âš ï¸ Net Income ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {str(e)}")
        
        # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        df = ticker_obj.history(start=start, end=end, interval="1d",
                               auto_adjust=False, actions=False, timeout=10)
        
        if df.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„° ë°˜í™˜ë¨: {ticker}")
            # ë¹ˆ ë°ì´í„°ë„ ìƒì¥ íì§€ ì¢…ëª©ìœ¼ë¡œ ì²˜ë¦¬ (ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
            
        print(f"[US] âœ… ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker} ({len(df)} í–‰)")
        df = df.rename_axis("date").reset_index()
        df["symbol"] = ticker
        return df
    except requests.exceptions.ReadTimeout:
        print(f"[US] â±ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {ticker} - API ì‘ë‹µ ì§€ì—° (ì¬ì‹œë„ í•„ìš”)")
        return None
    except requests.exceptions.ConnectionError:
        print(f"[US] ğŸŒ ì—°ê²° ì˜¤ë¥˜: {ticker} - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë°œìƒ (ì¬ì‹œë„ í•„ìš”)")
        return None
    except Exception as e:
        error_msg = str(e).lower()
        # ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ ë° ì²˜ë¦¬ (ê°ì§€ ì¡°ê±´ í™•ì¥)
        delisted_keywords = ["delisted", "no timezone found", "possibly delisted", "not found", 
                            "invalid ticker", "symbol may be delisted", "404"]
        if any(keyword in error_msg for keyword in delisted_keywords):
            print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
            # ë¹ˆ íŒŒì¼ ìƒì„±í•˜ì—¬ ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•Šë„ë¡ í•¨
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
        elif "rate limit" in error_msg or "429" in error_msg:
            print(f"[US] ğŸš« API ì œí•œ ë„ë‹¬: {ticker} - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
        else:
            print(f"[US] âŒ ì˜¤ë¥˜ ë°œìƒ ({ticker}): {error_msg[:100]}")
        return None

# ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „)
def fetch_and_save_us_ohlcv_chunked(tickers, save_dir=DATA_US_DIR, chunk_size=5, pause=5.0, start_chunk=0, max_chunks=None, max_workers=3):
    ensure_dir(save_dir)
    today = get_us_market_today()
    
    # ì§„í–‰ ìƒí™© ì €ì¥ íŒŒì¼
    progress_file = os.path.join(DATA_DIR, "fetch_progress.txt")
    
    # ì´ ì²­í¬ ìˆ˜ ê³„ì‚°
    total_chunks = (len(tickers) + chunk_size - 1) // chunk_size
    if max_chunks is not None:
        total_chunks = min(total_chunks, start_chunk + max_chunks)

    # ì‹œì‘ ì²­í¬ ì„¤ì •
    if start_chunk > 0:
        print(f"ğŸ”„ ì²­í¬ {start_chunk}ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤ (ì´ {total_chunks} ì²­í¬)")
    
    # í‹°ì»¤ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    def process_ticker(ticker):
        # Windows ì˜ˆì•½ íŒŒì¼ëª… ì²˜ë¦¬
        safe_ticker = safe_filename(ticker)
        path = os.path.join(save_dir, f"{safe_ticker}.csv")
        if os.path.exists(path):
            try:
                existing = pd.read_csv(path, parse_dates=["date"])
                if "date" not in existing.columns:
                    raise ValueError("âŒ 'date' ì»¬ëŸ¼ ì—†ìŒ")
                
                # ë‚ ì§œ ë°ì´í„°ë¥¼ UTCë¡œ ë³€í™˜
                existing["date"] = pd.to_datetime(existing["date"], utc=True)
                
                # ë¹ˆ íŒŒì¼ í™•ì¸ (ìƒì¥ íì§€ ì¢…ëª© í‘œì‹œìš©)
                if len(existing) == 0 and all(col in existing.columns for col in ["date", "symbol", "open", "high", "low", "close", "volume"]):
                    print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© (ì´ì „ì— í™•ì¸ë¨): {ticker}")
                    return False
                
                # ë‚ ì§œ ì»¬ëŸ¼ì´ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                    existing["date"] = pd.to_datetime(existing["date"], utc=True)
                    
                # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„°ê°€ 330ì¼ ì´ìƒì¸ ê²½ìš° ì˜¤ë˜ëœ ë°ì´í„° ì œê±°)
                if len(existing) > 330:
                    print(f"[US] âœ‚ï¸ {ticker}: 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(existing)} â†’ 330)")
                    existing = existing.sort_values("date", ascending=False).head(330).reset_index(drop=True)
                    # ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ë‹¤ì‹œ ì •ë ¬
                    existing = existing.sort_values("date", ascending=True).reset_index(drop=True)
                    
                last_date = existing["date"].dropna().max().date()
                start_date = last_date + timedelta(days=1)
            except Exception as e:
                print(f"âš ï¸ {ticker} ê¸°ì¡´ íŒŒì¼ ì˜¤ë¥˜: {e}")
                existing = None
                start_date = today - timedelta(days=450)
        else:
            existing = None
            start_date = today - timedelta(days=450)

        if start_date >= today:
            print(f"[US] â© ìµœì‹  ìƒíƒœ (ë˜ëŠ” ì˜¤ëŠ˜ì€ ê±°ë˜ì¼ ì•„ë‹˜): {ticker}")
            return False

        print(f"[DEBUG] {ticker}: ìˆ˜ì§‘ ì‹œì‘ì¼ {start_date}, ì¢…ë£Œì¼ {today}")

        df_new = None
        for j in range(5):  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            try:
                df_new = fetch_us_single(ticker, start=start_date, end=today)
                
                # ìƒì¥ íì§€ ì¢…ëª© ì²˜ë¦¬ (ë¹ˆ DataFrameì´ ë°˜í™˜ëœ ê²½ìš°)
                if df_new is not None and df_new.empty and len(df_new.columns) > 0:
                    print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                    df_new.to_csv(path, index=False)
                    return True
                elif df_new is not None and not df_new.empty:
                    # ì •ìƒ ë°ì´í„° íšë“
                    break
                    
                print(f"[US] âš ï¸ {ticker} ë¹ˆ ë°ì´í„° ë°˜í™˜, ì¬ì‹œë„ {j+1}/5")
            except Exception as e:
                error_msg = str(e).lower()
                # ìƒì¥ íì§€ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                delisted_keywords = ["delisted", "no timezone", "possibly delisted", "not found", "invalid ticker", "404"]
                if any(keyword in error_msg for keyword in delisted_keywords):
                    print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                    # ë¹ˆ íŒŒì¼ ìƒì„±
                    empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                    empty_df.to_csv(path, index=False)
                    return True
                    
                wait = 2 ** j + 2  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                print(f"[US] âš ï¸ {ticker} ì¬ì‹œë„ {j+1}/5 â†’ {wait}s ëŒ€ê¸°: {error_msg[:100]}")
                time.sleep(wait)
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¶”ê°€ ëŒ€ê¸°
            if j < 4:  # ë§ˆì§€ë§‰ ì‹œë„ ì „ê¹Œì§€
                time.sleep(1)  # API ìš”ì²­ ì‚¬ì´ì— ì¶”ê°€ ëŒ€ê¸°

        if df_new is None or df_new.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„°: {ticker}")
            # ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ë„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒì¥ íì§€ë¡œ ê°„ì£¼
            empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
            empty_df.to_csv(path, index=False)
            print(f"[US] ğŸš« ë°ì´í„° ì—†ìŒ - ìƒì¥ íì§€ë¡œ ì²˜ë¦¬: {ticker}")
            return True

        if existing is not None:
            before_len = len(existing)
            
            # ë‚ ì§œ í˜•ì‹ í†µì¼ (ëª¨ë“  ë‚ ì§œë¥¼ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜)
            if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                existing["date"] = pd.to_datetime(existing["date"], utc=True)
            if not pd.api.types.is_datetime64tz_dtype(df_new["date"]):
                df_new["date"] = pd.to_datetime(df_new["date"], utc=True)
                
            # ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±° (ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°)
            existing["date_str"] = existing["date"].dt.strftime("%Y-%m-%d")
            df_new["date_str"] = df_new["date"].dt.strftime("%Y-%m-%d")
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìƒˆ ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” ë‚ ì§œ ì œê±°
            existing_filtered = existing[~existing["date_str"].isin(df_new["date_str"])]
            
            # ë°ì´í„° ë³‘í•©
            df_combined = pd.concat([existing_filtered, df_new], ignore_index=True)
            
            # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
            df_combined.drop("date_str", axis=1, inplace=True)
            
            # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„° ë³‘í•© í›„ ë‹¤ì‹œ í™•ì¸)
            if len(df_combined) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ë³‘í•© í›„ 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(df_combined)} â†’ 330)")
                df_combined = df_combined.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_combined = df_combined.sort_values("date", ascending=True).reset_index(drop=True)
            
            after_len = len(df_combined)

            # í•­ìƒ ì €ì¥í•˜ì—¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ì¥
            df_combined.to_csv(path, index=False)
            if after_len > before_len:
                print(f"[US] âœ… ì €ì¥ë¨: {ticker} ({after_len} rows, +{after_len - before_len})")
            else:
                print(f"[US] ğŸ”„ ë°ì´í„° ì—…ë°ì´íŠ¸ë¨: {ticker} ({after_len} rows)")
            return True
        else:
            # ì‹ ê·œ ë°ì´í„°ë„ 330 ì˜ì—…ì¼ ì œí•œ ì ìš©
            if len(df_new) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ì‹ ê·œ ë°ì´í„° 330 ì˜ì—…ì¼ ì´ˆê³¼ ì •ë¦¬ ì¤‘ ({len(df_new)} â†’ 330)")
                df_new = df_new.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_new = df_new.sort_values("date", ascending=True).reset_index(drop=True)
                
            df_new.to_csv(path, index=False)
            print(f"[US] âœ… ì‹ ê·œ ì €ì¥: {ticker} ({len(df_new)} rows)")
            return True
    
    # ì²­í¬ë³„ ì²˜ë¦¬ ì‹œì‘
    for i in range(start_chunk * chunk_size, len(tickers), chunk_size):
        chunk_num = i // chunk_size
        
        # ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ í™•ì¸
        if max_chunks is not None and chunk_num >= start_chunk + max_chunks:
            print(f"ğŸ›‘ ìµœëŒ€ ì²­í¬ ìˆ˜ ({max_chunks}) ë„ë‹¬. ì‘ì—… ì¤‘ë‹¨.")
            break
            
        chunk = tickers[i:i+chunk_size]
        
        # NaN ê°’ í•„í„°ë§
        chunk = [t for t in chunk if isinstance(t, str) or (isinstance(t, (int, float)) and not pd.isna(t))]
        
        print(f"\nâ±ï¸ Chunk {chunk_num + 1}/{total_chunks} ì‹œì‘ ({len(chunk)}ê°œ): {chunk}")
        
        # ì§„í–‰ ìƒí™© ì €ì¥
        with open(progress_file, 'w') as f:
            f.write(str(chunk_num))
        
        # ì²­í¬ ë‚´ í‹°ì»¤ ë³‘ë ¬ ì²˜ë¦¬
        success_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë³‘ë ¬ë¡œ í‹°ì»¤ ì²˜ë¦¬ ì‹¤í–‰
            results = list(executor.map(process_ticker, chunk))
            success_count = sum(1 for result in results if result)
        
        # ì²­í¬ ì™„ë£Œ í›„ ìƒíƒœ ì¶œë ¥ ë° ëŒ€ê¸°
        print(f"âœ… ì²­í¬ {chunk_num + 1}/{total_chunks} ì™„ë£Œ: {success_count}/{len(chunk)} ì„±ê³µ")
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        if chunk_num + 1 < total_chunks:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
            print(f"â³ {pause}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(pause)

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

# ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
def collect_data(max_us_chunks=None, start_chunk=0):
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for directory in [DATA_DIR, DATA_US_DIR, RESULTS_DIR]:
        ensure_dir(directory)
        
    print("\nğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    us_tickers = get_or_load_cached_nasdaq_symbols()
    fetch_and_save_us_ohlcv_chunked(
        tickers=us_tickers,
        save_dir=DATA_US_DIR,
        chunk_size=5,
        pause=5.0,
        start_chunk=start_chunk,
        max_chunks=max_us_chunks,
        max_workers=3
    )

# ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Mark Minervini ìŠ¤í¬ë¦¬ë„ˆ - ë°ì´í„° ìˆ˜ì§‘")
    parser.add_argument("--max-us-chunks", type=int, help="ìµœëŒ€ ë¯¸êµ­ ì£¼ì‹ ì²­í¬ ìˆ˜ ì œí•œ")
    parser.add_argument("--start-chunk", type=int, default=0, help="ì‹œì‘í•  ì²­í¬ ë²ˆí˜¸")
    
    args = parser.parse_args()
    
    collect_data(
        max_us_chunks=args.max_us_chunks,
        start_chunk=args.start_chunk
    )