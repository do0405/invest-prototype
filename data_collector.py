#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Mark Minervini Screener - ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ

import os
import csv
import time
import requests
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
from pytz import timezone
from concurrent.futures import ThreadPoolExecutor

from utils import (
    ensure_dir, get_us_market_today, clean_tickers, safe_filename
)

from config import (
    DATA_DIR, DATA_US_DIR, RESULTS_DIR
)

# ì£¼ì‹ ì‹¬ë³¼ ìˆ˜ì§‘ (NASDAQ API ì œê±°ë¨ - íƒ€ì„ì•„ì›ƒ ë¬¸ì œë¡œ ì¸í•´)
# ëŒ€ì‹  Yahoo Financeë‚˜ ë‹¤ë¥¸ ì•ˆì •ì ì¸ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

# ë¯¸êµ­ ì£¼ì‹ ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def fetch_us_single(ticker, start, end):
    """yfinance APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ì¢…ëª©ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼
        start: ì‹œì‘ ë‚ ì§œ
        end: ì¢…ë£Œ ë‚ ì§œ
        
    Returns:
        DataFrame: ì£¼ê°€ ë°ì´í„° ë˜ëŠ” None(ì˜¤ë¥˜ ë°œìƒ ì‹œ)
    """
    try:
        # ìš”ì²­ ì „ ì§§ì€ ëŒ€ê¸° ì¶”ê°€ (API ì œí•œ ë°©ì§€)
        time.sleep(0.5)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
        print(f"[US] ğŸ“Š ë°ì´í„° ìš”ì²­ ì¤‘: {ticker} ({start} ~ {end})")
        ticker_obj = yf.Ticker(ticker)
        
        # income_stmtì—ì„œ Net Income ê°€ì ¸ì˜¤ê¸°
        try:
            income_stmt = ticker_obj.income_stmt
            if income_stmt is not None and not income_stmt.empty:
                net_income = income_stmt.loc['Net Income'] if 'Net Income' in income_stmt.index else None
                if net_income is not None:
                    print(f"[US] âœ… Net Income ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker}")
        except Exception as e:
            print(f"[US] âš ï¸ Net Income ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {ticker} - {str(e)}")
        
        # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        df = ticker_obj.history(start=start, end=end, interval="1d",
                               auto_adjust=False, actions=False, timeout=10)
        
        if df.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„° ë°˜í™˜ë¨: {ticker}")
            # ë¹ˆ ë°ì´í„°ë„ ìƒì¥ íì§€ ì¢…ëª©ìœ¼ë¡œ ì²˜ë¦¬ (ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
            
        print(f"[US] âœ… ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ: {ticker} ({len(df)} í–‰)")
        df = df.rename_axis("date").reset_index()
        df["symbol"] = ticker
        return df
    except requests.exceptions.ReadTimeout:
        print(f"[US] â±ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {ticker} - API ì‘ë‹µ ì§€ì—° (ì¬ì‹œë„ í•„ìš”)")
        return None
    except requests.exceptions.ConnectionError:
        print(f"[US] ğŸŒ ì—°ê²° ì˜¤ë¥˜: {ticker} - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë°œìƒ (ì¬ì‹œë„ í•„ìš”)")
        return None
    except Exception as e:
        error_msg = str(e).lower()
        # ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ ë° ì²˜ë¦¬ (ê°ì§€ ì¡°ê±´ í™•ì¥)
        delisted_keywords = ["delisted", "no timezone found", "possibly delisted", "not found", 
                            "invalid ticker", "symbol may be delisted", "404"]
        if any(keyword in error_msg for keyword in delisted_keywords):
            print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
            # ë¹ˆ íŒŒì¼ ìƒì„±í•˜ì—¬ ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•Šë„ë¡ í•¨
            return pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
        elif "rate limit" in error_msg or "429" in error_msg:
            print(f"[US] ğŸš« API ì œí•œ ë„ë‹¬: {ticker} - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
        else:
            print(f"[US] âŒ ì˜¤ë¥˜ ë°œìƒ ({ticker}): {error_msg[:100]}")
        return None

# ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „)
def fetch_and_save_us_ohlcv_chunked(tickers, save_dir=DATA_US_DIR, chunk_size=5, pause=5.0, start_chunk=0, max_chunks=None, max_workers=3):
    ensure_dir(save_dir)
    today = get_us_market_today()
    
    # ì§„í–‰ ìƒí™© ì €ì¥ íŒŒì¼
    progress_file = os.path.join(DATA_DIR, "fetch_progress.txt")
    
    # ì´ ì²­í¬ ìˆ˜ ê³„ì‚°
    total_chunks = (len(tickers) + chunk_size - 1) // chunk_size
    if max_chunks is not None:
        total_chunks = min(total_chunks, start_chunk + max_chunks)

    # ì‹œì‘ ì²­í¬ ì„¤ì •
    if start_chunk > 0:
        print(f"ğŸ”„ ì²­í¬ {start_chunk}ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤ (ì´ {total_chunks} ì²­í¬)")
    
    # í‹°ì»¤ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    def process_ticker(ticker):
        # Windows ì˜ˆì•½ íŒŒì¼ëª… ì²˜ë¦¬
        safe_ticker = safe_filename(ticker)
        path = os.path.join(save_dir, f"{safe_ticker}.csv")
        if os.path.exists(path):
            try:
                existing = pd.read_csv(path, parse_dates=["date"])
                if "date" not in existing.columns:
                    raise ValueError("âŒ 'date' ì»¬ëŸ¼ ì—†ìŒ")

                # ë‚ ì§œ ë°ì´í„°ë¥¼ UTCë¡œ ë³€í™˜
                existing["date"] = pd.to_datetime(existing["date"], utc=True)

                if len(existing) == 0:
                    # í—¤ë”ë§Œ ì¡´ì¬í•˜ëŠ” ë¹ˆ íŒŒì¼ì˜ ê²½ìš° ìƒˆë¡œ ìˆ˜ì§‘
                    print(f"[US] ğŸ“Š ë¹ˆ íŒŒì¼ ê°ì§€, ìƒˆë¡œ ë°ì´í„° ìˆ˜ì§‘: {ticker}")
                    existing = None
                    start_date = today - timedelta(days=450)
                else:
                    # ë‚ ì§œ ì»¬ëŸ¼ì´ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                        existing["date"] = pd.to_datetime(existing["date"], utc=True)

                    # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„°ê°€ 330ì¼ ì´ìƒì¸ ê²½ìš° ì˜¤ë˜ëœ ë°ì´í„° ì œê±°)
                    if len(existing) > 330:
                        print(f"[US] âœ‚ï¸ {ticker}: 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(existing)} â†’ 330)")
                        existing = existing.sort_values("date", ascending=False).head(330).reset_index(drop=True)
                        # ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ë‹¤ì‹œ ì •ë ¬
                        existing = existing.sort_values("date", ascending=True).reset_index(drop=True)

                    last_date = existing["date"].dropna().max().date()
                    start_date = last_date + timedelta(days=1)
            except Exception as e:
                print(f"âš ï¸ {ticker} ê¸°ì¡´ íŒŒì¼ ì˜¤ë¥˜: {e}")
                existing = None
                start_date = today - timedelta(days=450)
        else:
            existing = None
            start_date = today - timedelta(days=450)

        if start_date >= today:
            print(f"[US] â© ìµœì‹  ìƒíƒœ (ë˜ëŠ” ì˜¤ëŠ˜ì€ ê±°ë˜ì¼ ì•„ë‹˜): {ticker}")
            return False

        print(f"[DEBUG] {ticker}: ìˆ˜ì§‘ ì‹œì‘ì¼ {start_date}, ì¢…ë£Œì¼ {today}")

        df_new = None
        for j in range(5):  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            try:
                df_new = fetch_us_single(ticker, start=start_date, end=today)
                
                # ìƒì¥ íì§€ ì¢…ëª© ì²˜ë¦¬ (ë¹ˆ DataFrameì´ ë°˜í™˜ëœ ê²½ìš°)
                if df_new is not None and df_new.empty:
                    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìƒì¥íì§€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                    if existing is not None and len(existing) > 0:
                        print(f"[US] â© ìƒˆ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {ticker}")
                        return False
                    else:
                        print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                        # ë¹ˆ DataFrameì— ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì €ì¥, ì—†ìœ¼ë©´ ê¸°ë³¸ ì»¬ëŸ¼ ì¶”ê°€
                        if len(df_new.columns) == 0:
                            df_new = pd.DataFrame(columns=['date', 'symbol', 'open', 'high', 'low', 'close', 'volume'])
                        df_new.to_csv(path, index=False)
                        return True
                elif df_new is not None and not df_new.empty:
                    # ì •ìƒ ë°ì´í„° íšë“
                    break
                    
                print(f"[US] âš ï¸ {ticker} ë¹ˆ ë°ì´í„° ë°˜í™˜, ì¬ì‹œë„ {j+1}/5")
            except Exception as e:
                error_msg = str(e).lower()
                # ìƒì¥ íì§€ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                delisted_keywords = ["delisted", "no timezone", "possibly delisted", "not found", "invalid ticker", "404"]
                if any(keyword in error_msg for keyword in delisted_keywords):
                    print(f"[US] ğŸš« ìƒì¥ íì§€ ì¢…ëª© ê°ì§€ë¨: {ticker}")
                    # ë¹ˆ íŒŒì¼ ìƒì„±
                    empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                    empty_df.to_csv(path, index=False)
                    return True
                    
                wait = 2 ** j + 2  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                print(f"[US] âš ï¸ {ticker} ì¬ì‹œë„ {j+1}/5 â†’ {wait}s ëŒ€ê¸°: {error_msg[:100]}")
                time.sleep(wait)
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¶”ê°€ ëŒ€ê¸°
            if j < 4:  # ë§ˆì§€ë§‰ ì‹œë„ ì „ê¹Œì§€
                time.sleep(1)  # API ìš”ì²­ ì‚¬ì´ì— ì¶”ê°€ ëŒ€ê¸°

        if df_new is None or df_new.empty:
            print(f"[US] âŒ ë¹ˆ ë°ì´í„°: {ticker}")
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìƒì¥íì§€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            if existing is not None and len(existing) > 0:
                print(f"[US] â© ìƒˆ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {ticker}")
                return False
            else:
                # ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ë„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒì¥ íì§€ë¡œ ê°„ì£¼
                empty_df = pd.DataFrame(columns=["date", "symbol", "open", "high", "low", "close", "volume"])
                empty_df.to_csv(path, index=False)
                print(f"[US] ğŸš« ë°ì´í„° ì—†ìŒ - ìƒì¥ íì§€ë¡œ ì²˜ë¦¬: {ticker}")
                return True

        if existing is not None:
            before_len = len(existing)
            
            # ë‚ ì§œ í˜•ì‹ í†µì¼ (ëª¨ë“  ë‚ ì§œë¥¼ UTC ì‹œê°„ëŒ€ë¡œ ë³€í™˜)
            if not pd.api.types.is_datetime64tz_dtype(existing["date"]):
                existing["date"] = pd.to_datetime(existing["date"], utc=True)
            if not pd.api.types.is_datetime64tz_dtype(df_new["date"]):
                df_new["date"] = pd.to_datetime(df_new["date"], utc=True)
                
            # ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±° (ì‹œê°„ëŒ€ ë¬¸ì œ í•´ê²°)
            existing["date_str"] = existing["date"].dt.strftime("%Y-%m-%d")
            df_new["date_str"] = df_new["date"].dt.strftime("%Y-%m-%d")
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìƒˆ ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” ë‚ ì§œ ì œê±°
            existing_filtered = existing[~existing["date_str"].isin(df_new["date_str"])]
            
            # ë°ì´í„° ë³‘í•©
            df_combined = pd.concat([existing_filtered, df_new], ignore_index=True)
            
            # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
            df_combined.drop("date_str", axis=1, inplace=True)
            
            # 330 ì˜ì—…ì¼ ì œí•œ ì ìš© (ë°ì´í„° ë³‘í•© í›„ ë‹¤ì‹œ í™•ì¸)
            if len(df_combined) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ë³‘í•© í›„ 330 ì˜ì—…ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬ ì¤‘ ({len(df_combined)} â†’ 330)")
                df_combined = df_combined.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_combined = df_combined.sort_values("date", ascending=True).reset_index(drop=True)
            
            after_len = len(df_combined)

            # í•­ìƒ ì €ì¥í•˜ì—¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ì¥
            df_combined.to_csv(path, index=False)
            if after_len > before_len:
                print(f"[US] âœ… ì €ì¥ë¨: {ticker} ({after_len} rows, +{after_len - before_len})")
            else:
                print(f"[US] ğŸ”„ ë°ì´í„° ì—…ë°ì´íŠ¸ë¨: {ticker} ({after_len} rows)")
            return True
        else:
            # ì‹ ê·œ ë°ì´í„°ë„ 330 ì˜ì—…ì¼ ì œí•œ ì ìš©
            if len(df_new) > 330:
                print(f"[US] âœ‚ï¸ {ticker}: ì‹ ê·œ ë°ì´í„° 330 ì˜ì—…ì¼ ì´ˆê³¼ ì •ë¦¬ ì¤‘ ({len(df_new)} â†’ 330)")
                df_new = df_new.sort_values("date", ascending=False).head(330).reset_index(drop=True)
            
            # ìµœì¢… ì €ì¥ ì „ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ìœ„ì— ì˜¤ë„ë¡ ì •ë ¬
            df_new = df_new.sort_values("date", ascending=True).reset_index(drop=True)
                
            df_new.to_csv(path, index=False)
            print(f"[US] âœ… ì‹ ê·œ ì €ì¥: {ticker} ({len(df_new)} rows)")
            return True
    
    # ì²­í¬ë³„ ì²˜ë¦¬ ì‹œì‘
    for i in range(start_chunk * chunk_size, len(tickers), chunk_size):
        chunk_num = i // chunk_size
        
        # ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ í™•ì¸
        if max_chunks is not None and chunk_num >= start_chunk + max_chunks:
            print(f"ğŸ›‘ ìµœëŒ€ ì²­í¬ ìˆ˜ ({max_chunks}) ë„ë‹¬. ì‘ì—… ì¤‘ë‹¨.")
            break
            
        chunk = tickers[i:i+chunk_size]
        
        # NaN ê°’ í•„í„°ë§
        chunk = [t for t in chunk if isinstance(t, str) or (isinstance(t, (int, float)) and not pd.isna(t))]
        
        print(f"\nâ±ï¸ Chunk {chunk_num + 1}/{total_chunks} ì‹œì‘ ({len(chunk)}ê°œ): {chunk}")
        
        # ì§„í–‰ ìƒí™© ì €ì¥
        with open(progress_file, 'w') as f:
            f.write(str(chunk_num))
        
        # ì²­í¬ ë‚´ í‹°ì»¤ ë³‘ë ¬ ì²˜ë¦¬
        success_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë³‘ë ¬ë¡œ í‹°ì»¤ ì²˜ë¦¬ ì‹¤í–‰
            results = list(executor.map(process_ticker, chunk))
            success_count = sum(1 for result in results if result)
        
        # ì²­í¬ ì™„ë£Œ í›„ ìƒíƒœ ì¶œë ¥ ë° ëŒ€ê¸°
        print(f"âœ… ì²­í¬ {chunk_num + 1}/{total_chunks} ì™„ë£Œ: {success_count}/{len(chunk)} ì„±ê³µ")
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        if chunk_num + 1 < total_chunks:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
            print(f"â³ {pause}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(pause)

# í¬ë¼ì¼„ ê´€ë ¨ í•¨ìˆ˜ ì œê±°ë¨

# ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
def collect_data(max_us_chunks=None, start_chunk=0):
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    for directory in [DATA_DIR, DATA_US_DIR, RESULTS_DIR]:
        ensure_dir(directory)
        
    print("\nğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # ê¸°ì¡´ CSV íŒŒì¼ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì¢…ëª© ëª©ë¡ ìƒì„±
    try:
        from data_collectors.stock_metadata_collector import get_symbols
        us_tickers = get_symbols()
        
        if not us_tickers:
            print("âš ï¸ ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            us_tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "META", "NFLX", "AMD", "CRM"]
        
        print(f"ğŸ“Š ì´ {len(us_tickers)}ê°œ ì¢…ëª©ì˜ OHLCV ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
        
        # OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        fetch_and_save_us_ohlcv_chunked(
            tickers=us_tickers,
            save_dir=DATA_US_DIR,
            chunk_size=10,
            pause=2.0,
            start_chunk=start_chunk,
            max_chunks=max_us_chunks,
            max_workers=5
        )
        
    except Exception as e:
        print(f"âŒ OHLCV ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("âš ï¸ ê¸°ë³¸ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¡œ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        us_tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "META", "NFLX", "AMD", "CRM"]
        fetch_and_save_us_ohlcv_chunked(
            tickers=us_tickers,
            save_dir=DATA_US_DIR,
            chunk_size=5,
            pause=3.0,
            start_chunk=start_chunk,
            max_chunks=max_us_chunks,
            max_workers=3
        )

# ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Mark Minervini ìŠ¤í¬ë¦¬ë„ˆ - ë°ì´í„° ìˆ˜ì§‘")
    parser.add_argument("--max-us-chunks", type=int, help="ìµœëŒ€ ë¯¸êµ­ ì£¼ì‹ ì²­í¬ ìˆ˜ ì œí•œ")
    parser.add_argument("--start-chunk", type=int, default=0, help="ì‹œì‘í•  ì²­í¬ ë²ˆí˜¸")
    
    args = parser.parse_args()
    
    collect_data(
        max_us_chunks=args.max_us_chunks,
        start_chunk=args.start_chunk
    )