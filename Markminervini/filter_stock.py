import pandas as pd
import os
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# configì—ì„œ í•„ìš”í•œ ê²½ë¡œ ì„í¬íŠ¸
from config import (
    RESULTS_DIR, US_WITH_RS_PATH, 
    DATA_DIR, DATA_US_DIR,
    ADVANCED_FINANCIAL_RESULTS_PATH
)

def ensure_dir(path):
    """ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    os.makedirs(path, exist_ok=True)

# CSV ë³‘ë ¬ ë¡œë”© í•¨ìˆ˜
def load_csvs_parallel(directory, time_col, id_col, max_workers=6):
    """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        directory: CSV íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        time_col: ì‹œê°„ ì»¬ëŸ¼ëª…
        id_col: ID ì»¬ëŸ¼ëª…
        max_workers: ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜
        
    Returns:
        list: ë¡œë“œëœ DataFrame ë¦¬ìŠ¤íŠ¸
    """
    files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    paths = [os.path.join(directory, f) for f in files]

    def _read_csv(path):
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(path)
            
            # ì»¬ëŸ¼ëª… ì†Œë¬¸ìë¡œ ë³€í™˜ (ëŒ€ì†Œë¬¸ì ì¼ê´€ì„± ìœ ì§€)
            df.columns = [col.lower() for col in df.columns]
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            time_col_lower = time_col.lower()
            if time_col_lower in df.columns:
                df[time_col_lower] = pd.to_datetime(df[time_col_lower])
            elif time_col.upper() in df.columns:
                # ëŒ€ë¬¸ì ì»¬ëŸ¼ëª…ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                df.rename(columns={time_col.upper(): time_col_lower}, inplace=True)
                df[time_col_lower] = pd.to_datetime(df[time_col_lower])
            elif 'date' in df.columns and time_col_lower == 'time':
                # í¬ë¦½í†  ë°ì´í„°ì—ì„œ 'time' ëŒ€ì‹  'date'ê°€ ìˆëŠ” ê²½ìš°
                df.rename(columns={'date': time_col_lower}, inplace=True)
                df[time_col_lower] = pd.to_datetime(df[time_col_lower])
            elif 'time' in df.columns and time_col_lower == 'date':
                # ì£¼ì‹ ë°ì´í„°ì—ì„œ 'date' ëŒ€ì‹  'time'ì´ ìˆëŠ” ê²½ìš°
                df.rename(columns={'time': time_col_lower}, inplace=True)
                df[time_col_lower] = pd.to_datetime(df[time_col_lower])
            
            # ID ì»¬ëŸ¼ ì²˜ë¦¬
            id_col_lower = id_col.lower()
            name = os.path.splitext(os.path.basename(path))[0]
            df[id_col_lower] = name
            
            # ì¸ë±ìŠ¤ ì„¤ì •
            if time_col_lower in df.columns:
                df = df.set_index(time_col_lower)
            else:
                print(f"âš ï¸ ì‹œê°„ ì»¬ëŸ¼ '{time_col_lower}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            
            return df
        except Exception as e:
            print(f"âŒ Error reading {path}: {e}")
            return None

    with ThreadPoolExecutor(max_workers=max_workers) as pool:
        dfs = list(pool.map(_read_csv, paths))
    
    # None ê°’ ì œê±°
    return [df for df in dfs if df is not None]

# í¬ë¦½í†  í•„í„°ë§ í•¨ìˆ˜ ì œê±°ë¨

def filter_us():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_file = os.path.join(RESULTS_DIR, 'us_with_rs.csv')
    
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv(input_file)
    
    # ëª¨ë“  ì¡°ê±´ì´ Trueì¸ ì¢…ëª©ë§Œ í•„í„°ë§
    conditions = [f'cond{i}' for i in range(1, 9)]  # cond1ë¶€í„° cond8ê¹Œì§€
    filtered_df = df[df[conditions].all(axis=1)]
    
    # RS ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_df = filtered_df.sort_values(by='rs_score', ascending=False)
    
    # ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì— ë®ì–´ì“°ê¸°
    sorted_df.to_csv(input_file, index=False)
    # JSON íŒŒì¼ ìƒì„± ì¶”ê°€
    json_file = input_file.replace('.csv', '.json')
    sorted_df.to_json(json_file, orient='records', indent=2, force_ascii=False)
    
    # í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
    print(f'[US] ì´ {len(df)}ê°œ ì¤‘ {len(filtered_df)}ê°œ ì¢…ëª©ì´ ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•¨')
    print('[US] ìƒìœ„ 5ê°œ ì¢…ëª©:')
    print(sorted_df[['symbol', 'rs_score']].head(5))

def run_integrated_screening():
    """
    ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ê³¼ ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” í•¨ìˆ˜
    """
    print("\nğŸ” í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹œì‘...")
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ (results í´ë”ì— ì €ì¥)
    from config import RESULTS_DIR
    INTEGRATED_RESULTS_PATH = os.path.join(RESULTS_DIR, 'integrated_results.csv')
    
    try:
        # ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ
        if not os.path.exists(US_WITH_RS_PATH):
            print(f"âš ï¸ ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {US_WITH_RS_PATH}")
            return
        
        tech_df = pd.read_csv(US_WITH_RS_PATH)
        print(f"âœ… ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(tech_df)}ê°œ ì¢…ëª©")
        
        # ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ
        if not os.path.exists(ADVANCED_FINANCIAL_RESULTS_PATH):
            print(f"âš ï¸ ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ADVANCED_FINANCIAL_RESULTS_PATH}")
            return
        
        fin_df = pd.read_csv(ADVANCED_FINANCIAL_RESULTS_PATH)
        print(f"âœ… ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(fin_df)}ê°œ ì¢…ëª©")
        
        # ë‘ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì»¬ëŸ¼ ì²˜ë¦¬)
        # 'rs_score' ì»¬ëŸ¼ì´ ì¤‘ë³µë  ê²½ìš° tech_dfì˜ ê²ƒì„ ì‚¬ìš©
        if 'rs_score' in tech_df.columns and 'rs_score' in fin_df.columns:
            fin_df = fin_df.rename(columns={'rs_score': 'rs_score_fin'})
        
        merged_df = pd.merge(tech_df, fin_df, on='symbol', how='inner')
        
        # ì¤‘ë³µ í–‰ ì œê±°
        merged_df = merged_df.drop_duplicates(subset=['symbol'])
        print(f"âœ… í†µí•© ê²°ê³¼: {len(merged_df)}ê°œ ì¢…ëª©")
        
        if merged_df.empty:
            print("âŒ í†µí•© ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê¸°ìˆ ì  ì¡°ê±´ ì¶©ì¡± ìˆ˜ì™€ ì¬ë¬´ì œí‘œ ì¡°ê±´ ì¶©ì¡± ìˆ˜ í•©ì‚°
        if 'met_count' in merged_df.columns and 'fin_met_count' in merged_df.columns:
            merged_df['total_met_count'] = merged_df['met_count'] + merged_df['fin_met_count']
        else:
            print(f"âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(merged_df.columns.tolist())}")
            if 'met_count' not in merged_df.columns:
                print("âŒ 'met_count' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            if 'fin_met_count' not in merged_df.columns:
                print("âŒ 'fin_met_count' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # RS ì ìˆ˜ ì—´ ì²˜ë¦¬
        if 'rs_score_x' in merged_df.columns:
            merged_df['rs_score'] = merged_df['rs_score_x']
            merged_df = merged_df.drop('rs_score_x', axis=1)
        if 'rs_score_y' in merged_df.columns:
            merged_df = merged_df.drop('rs_score_y', axis=1)
        
        # ê²°ê³¼ ì •ë ¬ ë° ì €ì¥ (total_met_count ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ì •ë ¬í•˜ê³ , ê·¸ ë‹¤ìŒ rs_scoreë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
        merged_df = merged_df.sort_values(['total_met_count', 'rs_score'], ascending=[False, False])
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì €ì¥ (symbol, met_count, fin_met_count, total_met_count, rs_score)
        selected_columns = ['symbol', 'met_count', 'fin_met_count', 'total_met_count', 'rs_score']
        filtered_df = merged_df[selected_columns]
        
        ensure_dir(RESULTS_DIR)
        filtered_df.to_csv(INTEGRATED_RESULTS_PATH, index=False)
        # JSON íŒŒì¼ ìƒì„± ì¶”ê°€
        json_path = INTEGRATED_RESULTS_PATH.replace('.csv', '.json')
        filtered_df.to_json(json_path, orient='records', indent=2, force_ascii=False)
        
        print(f"âœ… í†µí•© ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {INTEGRATED_RESULTS_PATH}")
        print(f"âœ… ì €ì¥ëœ ì»¬ëŸ¼: {', '.join(selected_columns)}")
        
        # ìƒìœ„ 10ê°œ ì¢…ëª© ì¶œë ¥
        top_10 = filtered_df.head(10).reset_index(drop=True)
        print("\nğŸ† í†µí•© ìŠ¤í¬ë¦¬ë‹ ìƒìœ„ 10ê°œ ì¢…ëª©:")
        pd.set_option('display.max_rows', None)
        print(top_10.to_string(index=True))
        
    except Exception as e:
        print(f"âŒ í†µí•© ìŠ¤í¬ë¦¬ë‹ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def main():
    parser = argparse.ArgumentParser(description='ì£¼ì‹ ë°ì´í„° í•„í„°ë§ ë° ì •ë ¬ ë„êµ¬')
    parser.add_argument('--integrated', action='store_true', help='í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰ (ê¸°ìˆ ì  + ì¬ë¬´ì œí‘œ)')
    args = parser.parse_args()
    
    if args.integrated:
        # í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰
        run_integrated_screening()
        return
    
    # ê¸°ë³¸ì ìœ¼ë¡œ US ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬
    filter_us()

if __name__ == '__main__':
    main()