#!/usr/bin/env python3
"""
ê³ ê¸‰ IPO ë°ì´í„° ìˆ˜ì§‘ê¸° - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ IPO ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë“ˆí™”ëœ ë°ì´í„° ì†ŒìŠ¤ ì§€ì›
- ê³¼ê±° ë° ì˜ˆì •ëœ IPO ë°ì´í„° ëª¨ë‘ ìˆ˜ì§‘
- CSV ë° JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬
"""

import logging
import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pandas as pd
import requests
from pathlib import Path

# ëª¨ë“ˆí™”ëœ ë°ì´í„° ì†ŒìŠ¤ë“¤
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from data_sources.sec_edgar_source import SecEdgarSource
    print("âœ… SecEdgar ë°ì´í„° ì†ŒìŠ¤ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ SecEdgar Import ì˜¤ë¥˜: {e}")
    # ê¸°ë³¸ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš©
    from data_sources.base_source import BaseDataSource
    
    SecEdgarSource = BaseDataSource
    print("âš ï¸ BaseDataSource ì‚¬ìš©")

# ë¡œê¹… ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True  # ê¸°ì¡´ ì„¤ì •ì„ ë®ì–´ì”€
)
logger = logging.getLogger(__name__)

class RealIPODataCollector:
    """ì‹¤ì œ IPO ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, data_dir: str = "../../data/IPO"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ì†ŒìŠ¤ë“¤ ì´ˆê¸°í™” (SecEdgarë§Œ ì‚¬ìš©)
        self.sources = [
            SecEdgarSource()  # SEC Edgar ì†ŒìŠ¤ë§Œ ì‚¬ìš©
        ]
        
        logger.info("ì‹¤ì œ IPO ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ê¸°ì¡´ ê°œë³„ ë©”ì„œë“œë“¤ì€ ëª¨ë“ˆí™”ëœ ë°ì´í„° ì†ŒìŠ¤ë¡œ ì´ë™ë¨
    
    def _clean_and_deduplicate(self, ipos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """IPO ë°ì´í„° ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°"""
        if not ipos:
            return []
        
        # ì‹¬ë³¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ìš°ì„ ìˆœìœ„: finance_calendars > investpy)
        seen_symbols = set()
        cleaned_ipos = []
        
        # finance_calendars ë°ì´í„° ìš°ì„  ì²˜ë¦¬
        for ipo in ipos:
            symbol = ipo.get('symbol', 'N/A')
            if symbol != 'N/A' and symbol not in seen_symbols:
                seen_symbols.add(symbol)
                cleaned_ipos.append(ipo)
            elif symbol == 'N/A':
                # ì‹¬ë³¼ì´ ì—†ëŠ” ê²½ìš° íšŒì‚¬ëª…ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                company_name = ipo.get('company_name', 'N/A')
                if company_name not in [c.get('company_name') for c in cleaned_ipos]:
                    cleaned_ipos.append(ipo)
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ {len(cleaned_ipos)}ê°œ IPO ë°ì´í„°")
        return cleaned_ipos
    
    def _save_to_files(self, data: List[Dict[str, Any]], file_prefix: str) -> Dict[str, str]:
        """ë°ì´í„°ë¥¼ CSVì™€ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            logger.warning(f"{file_prefix} ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV íŒŒì¼ ì €ì¥
        csv_filename = f"{file_prefix}_{timestamp}.csv"
        csv_path = self.data_dir / csv_filename
        
        df = pd.DataFrame(data)
        df.to_csv(csv_path, index=False, encoding='utf-8')
        logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path} ({len(data)}ê°œ IPO)")
        
        # JSON íŒŒì¼ ì €ì¥
        json_filename = f"{file_prefix}_{timestamp}.json"
        json_path = self.data_dir / json_filename
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_path} ({len(data)}ê°œ IPO)")
        
        return {
            'csv': str(csv_path),
            'json': str(json_path)
        }
    
    def collect_all_ipo_data(self) -> Dict[str, Any]:
        """ëª¨ë“  IPO ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
        logger.info("ì‹¤ì œ IPO ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # ìµœê·¼ IPO ë°ì´í„° ìˆ˜ì§‘ (ëª¨ë“  ì†ŒìŠ¤ í†µí•©)
        recent_ipos = []
        upcoming_ipos = []
        
        for source in self.sources:
            source_name = source.__class__.__name__
            try:
                # ì†ŒìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                if hasattr(source, 'is_available'):
                    available = source.is_available()
                    logger.info(f"{source_name} ì‚¬ìš© ê°€ëŠ¥: {available}")
                    if not available:
                        continue
                
                # ë°ì´í„° ìˆ˜ì§‘ ì‹œë„
                logger.info(f"{source_name}ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                recent_data = source.get_recent_ipos(months_back=3)
                upcoming_data = source.get_upcoming_ipos(months_ahead=3)
                
                if recent_data:
                    recent_ipos.extend(recent_data)
                    logger.info(f"{source_name}: ìµœê·¼ IPO {len(recent_data)}ê°œ ìˆ˜ì§‘")
                
                if upcoming_data:
                    upcoming_ipos.extend(upcoming_data)
                    logger.info(f"{source_name}: ì˜ˆì • IPO {len(upcoming_data)}ê°œ ìˆ˜ì§‘")
                    
                if not recent_data and not upcoming_data:
                    logger.warning(f"{source_name}: ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                logger.error(f"{source_name} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        # ë°ì´í„° ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
        recent_ipos = self._clean_and_deduplicate(recent_ipos)
        upcoming_ipos = self._clean_and_deduplicate(upcoming_ipos)
        
        # íŒŒì¼ ì €ì¥
        recent_files = self._save_to_files(recent_ipos, 'recent_ipos')
        upcoming_files = self._save_to_files(upcoming_ipos, 'upcoming_ipos')
        
        logger.info("ì „ì²´ IPO ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ")
        
        return {
            'recent_ipos': recent_ipos,
            'upcoming_ipos': upcoming_ipos,
            'files': {
                'recent': recent_files,
                'upcoming': upcoming_files
            }
        }

    def get_recent_ipos(self, days_back: int = 365) -> pd.DataFrame:
        """ìµœê·¼ IPO ë°ì´í„°ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        csv_files = sorted(self.data_dir.glob('recent_ipos_*.csv'))
        if not csv_files:
            return pd.DataFrame()

        df_list = []
        for file in csv_files:
            df = pd.read_csv(file)
            df.columns = [c.lower() for c in df.columns]
            if 'ticker' in df.columns and 'symbol' not in df.columns:
                df.rename(columns={'ticker': 'symbol'}, inplace=True)
            if 'date' in df.columns and 'ipo_date' not in df.columns:
                df.rename(columns={'date': 'ipo_date'}, inplace=True)
            if 'price_range' in df.columns and 'ipo_price' not in df.columns:
                df['ipo_price'] = (
                    df['price_range']
                    .astype(str)
                    .str.replace('$', '')
                    .str.split('-')
                    .str[0]
                    .str.replace(',', '')
                )
            df_list.append(df)

        df_all = pd.concat(df_list, ignore_index=True)
        if 'ipo_date' in df_all.columns:
            df_all['ipo_date'] = pd.to_datetime(df_all['ipo_date'], errors='coerce')
            cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=days_back)
            df_all = df_all[df_all['ipo_date'] >= cutoff]
        return df_all

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = RealIPODataCollector()
    
    try:
        results = collector.collect_all_ipo_data()
        
        print("\n=== ìˆ˜ì§‘ ê²°ê³¼ ===")
        print(f"ê³¼ê±° IPO ë°ì´í„°: {len(results['recent_ipos'])}ê°œ")
        print(f"ì˜ˆì •ëœ IPO ë°ì´í„°: {len(results['upcoming_ipos'])}ê°œ")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if results['recent_ipos']:
            print("\n=== ìµœê·¼ IPO ìƒ˜í”Œ ===")
            for ipo in results['recent_ipos'][:3]:
                symbol = ipo.get('symbol', 'N/A')
                company = ipo.get('company_name', 'N/A')
                date = ipo.get('ipo_date', 'N/A')
                print(f"- {symbol}: {company} ({date})")
        
        if results['upcoming_ipos']:
            print("\n=== ì˜ˆì •ëœ IPO ìƒ˜í”Œ ===")
            for ipo in results['upcoming_ipos'][:3]:
                symbol = ipo.get('symbol', 'N/A')
                company = ipo.get('company_name', 'N/A')
                date = ipo.get('expected_ipo_date', 'N/A')
                print(f"- {symbol}: {company} ({date})")
        
            
    except Exception as e:
        logger.error(f"IPO ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    try:
        collector = RealIPODataCollector()
        results = collector.collect_all_ipo_data()
        
        print(f"\nâœ… IPO ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœê·¼ IPO: {len(results['recent_ipos'])}ê°œ")
        print(f"ğŸ“… ì˜ˆì •ëœ IPO: {len(results['upcoming_ipos'])}ê°œ")
        
        # ì €ì¥ëœ íŒŒì¼ ì •ë³´
        print("\n=== ì €ì¥ëœ íŒŒì¼ ===")
        files = results['files']
        if files.get('recent'):
            print(f"- recent_csv: {files['recent']['csv']}")
            print(f"- recent_json: {files['recent']['json']}")
        if files.get('upcoming'):
            print(f"- upcoming_csv: {files['upcoming']['csv']}")
            print(f"- upcoming_json: {files['upcoming']['json']}")
            
    except Exception as e:
        logger.error(f"IPO ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
