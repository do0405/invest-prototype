#!/usr/bin/env python3
"""
ê³ ê¸‰ IPO ë°ì´í„° ìˆ˜ì§‘ê¸° - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ IPO ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë“ˆí™”ëœ ë°ì´í„° ì†ŒìŠ¤ ì§€ì›
- ê³¼ê±° ë° ì˜ˆì •ëœ IPO ë°ì´í„° ëª¨ë‘ ìˆ˜ì§‘
- CSV ë° JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬
"""

import logging
import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pandas as pd
import requests
from pathlib import Path

# SEC Edgar ë°ì´í„° ì†ŒìŠ¤ë§Œ ì‚¬ìš©
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_sources.sec_edgar_source import SecEdgarSource

# ë¡œê¹… ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True  # ê¸°ì¡´ ì„¤ì •ì„ ë®ì–´ì”€
)
logger = logging.getLogger(__name__)

class RealIPODataCollector:
    """ì‹¤ì œ IPO ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, data_dir: str = None):
        # ê¸°ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
        if data_dir is None:
            # configì—ì„œ DATA_DIR ê°€ì ¸ì˜¤ê¸°
            try:
                sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
                from config import DATA_DIR
                data_dir = os.path.join(DATA_DIR, 'IPO')
            except ImportError:
                data_dir = "../../data/IPO"
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # SEC Edgar ë°ì´í„° ì†ŒìŠ¤ë§Œ ì‚¬ìš©
        self.sec_edgar_source = SecEdgarSource()
        
        logger.info(f"IPO ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ: {self.data_dir}")
    
    # ê¸°ì¡´ ê°œë³„ ë©”ì„œë“œë“¤ì€ ëª¨ë“ˆí™”ëœ ë°ì´í„° ì†ŒìŠ¤ë¡œ ì´ë™ë¨
    
    def _clean_and_deduplicate(self, ipos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """IPO ë°ì´í„° ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°"""
        if not ipos:
            return []
        
        # ì‹¬ë³¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ìš°ì„ ìˆœìœ„: finance_calendars > investpy)
        seen_symbols = set()
        cleaned_ipos = []
        
        # finance_calendars ë°ì´í„° ìš°ì„  ì²˜ë¦¬
        for ipo in ipos:
            symbol = ipo.get('symbol', 'N/A')
            if symbol != 'N/A' and symbol not in seen_symbols:
                seen_symbols.add(symbol)
                cleaned_ipos.append(ipo)
            elif symbol == 'N/A':
                # ì‹¬ë³¼ì´ ì—†ëŠ” ê²½ìš° íšŒì‚¬ëª…ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                company_name = ipo.get('company_name', 'N/A')
                if company_name not in [c.get('company_name') for c in cleaned_ipos]:
                    cleaned_ipos.append(ipo)
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ {len(cleaned_ipos)}ê°œ IPO ë°ì´í„°")
        return cleaned_ipos
    
    def _save_to_files(self, data: List[Dict[str, Any]], file_prefix: str) -> Dict[str, str]:
        """ë°ì´í„°ë¥¼ CSVì™€ JSON íŒŒì¼ë¡œ ì €ì¥ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›, 1ë…„ ì´ìƒ ëœ ë°ì´í„° ìë™ ì •ë¦¬)"""
        if not data:
            logger.warning(f"{file_prefix} ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        # ë‚ ì§œë§Œ í¬í•¨í•œ íŒŒì¼ëª… ì‚¬ìš© (ì‹œê°„ ì •ë³´ ì œê±°)
        date_str = datetime.now().strftime('%Y%m%d')
        csv_filename = f"{file_prefix}_{date_str}.csv"
        json_filename = f"{file_prefix}_{date_str}.json"
        csv_path = self.data_dir / csv_filename
        json_path = self.data_dir / json_filename
        
        new_df = pd.DataFrame(data)
        
        # 1ë…„ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚° (pandas Timestampë¡œ ë³€í™˜)
        one_year_ago = pd.Timestamp(datetime.now() - timedelta(days=365))
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if csv_path.exists():
            try:
                existing_df = pd.read_csv(csv_path)
                
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œ 1ë…„ ì´ìƒ ëœ ë°ì´í„° ì œê±°
                if 'ipo_date' in existing_df.columns:
                    existing_df['ipo_date'] = pd.to_datetime(existing_df['ipo_date'], errors='coerce')
                    before_filter = len(existing_df)
                    # NaT ê°’ ì œê±° í›„ ë‚ ì§œ ë¹„êµ
                    valid_dates_mask = existing_df['ipo_date'].notna()
                    existing_df = existing_df[valid_dates_mask & (existing_df['ipo_date'] >= one_year_ago)]
                    after_filter = len(existing_df)
                    if before_filter > after_filter:
                        logger.info(f"ğŸ—‘ï¸ 1ë…„ ì´ìƒ ëœ IPO ë°ì´í„° {before_filter - after_filter}ê°œ ì œê±°")
                
                # ê¸°ë³¸ í‚¤ ì»¬ëŸ¼ í™•ì¸ (symbol ë˜ëŠ” ticker)
                key_col = None
                for col in ['symbol', 'ticker', 'company_name']:
                    if col in new_df.columns:
                        key_col = col
                        break
                
                if key_col and key_col in existing_df.columns:
                    # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìƒˆ ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” í•­ëª© ì œê±°
                    existing_df = existing_df[~existing_df[key_col].isin(new_df[key_col])]
                    
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•©
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    
                    # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
                    date_cols = [col for col in combined_df.columns if 'date' in col.lower()]
                    if date_cols:
                        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜ í›„ ì •ë ¬
                        sort_col = date_cols[0]
                        combined_df[sort_col] = pd.to_datetime(combined_df[sort_col], errors='coerce')
                        # NaT ê°’ì„ ë§ˆì§€ë§‰ì— ë°°ì¹˜í•˜ì—¬ ì •ë ¬
                        combined_df = combined_df.sort_values(sort_col, ascending=False, na_position='last')
                    
                    final_df = combined_df
                    logger.info(f"ğŸ”„ ì¦ë¶„ ì—…ë°ì´íŠ¸: ê¸°ì¡´ {len(existing_df)}ê°œ + ì‹ ê·œ {len(new_df)}ê°œ = ì´ {len(final_df)}ê°œ IPO")
                else:
                    final_df = new_df
                    logger.info(f"âš ï¸ í‚¤ ì»¬ëŸ¼ ë¶ˆì¼ì¹˜, ì „ì²´ êµì²´: {len(new_df)}ê°œ IPO")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({e}), ì „ì²´ êµì²´: {len(new_df)}ê°œ IPO")
                final_df = new_df
        else:
            final_df = new_df
            logger.info(f"âœ… ì‹ ê·œ ì €ì¥: {len(new_df)}ê°œ IPO")
        
        # ìµœì¢… ë°ì´í„°ì—ì„œë„ 1ë…„ ì´ìƒ ëœ ë°ì´í„° í•„í„°ë§
        if 'ipo_date' in final_df.columns:
            final_df['ipo_date'] = pd.to_datetime(final_df['ipo_date'], errors='coerce')
            before_final_filter = len(final_df)
            # NaT ê°’ ì œê±° í›„ ë‚ ì§œ ë¹„êµ
            valid_dates_mask = final_df['ipo_date'].notna()
            final_df = final_df[valid_dates_mask & (final_df['ipo_date'] >= one_year_ago)]
            after_final_filter = len(final_df)
            if before_final_filter > after_final_filter:
                logger.info(f"ğŸ“… ìµœì¢… ì €ì¥ ì „ 1ë…„ ì´ìƒ ëœ ë°ì´í„° {before_final_filter - after_final_filter}ê°œ ì¶”ê°€ ì œê±°")
        
        # CSV íŒŒì¼ ì €ì¥
        final_df.to_csv(csv_path, index=False, encoding='utf-8')
        logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path} (ìµœê·¼ 1ë…„ ë°ì´í„° {len(final_df)}ê°œ)")
        
        # JSON íŒŒì¼ ì¦ë¶„ ì—…ë°ì´íŠ¸
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_json = json.load(f)
                
                # JSONë„ ë™ì¼í•˜ê²Œ ì¦ë¶„ ì—…ë°ì´íŠ¸
                key_col = None
                for col in ['symbol', 'ticker', 'company_name']:
                    if col in data[0]:
                        key_col = col
                        break
                
                if key_col:
                    existing_keys = {item.get(key_col) for item in existing_json if key_col in item}
                    # ê¸°ì¡´ í•­ëª©ì—ì„œ ì—…ë°ì´íŠ¸ëœ í•­ëª© ì œê±°
                    updated_existing = [item for item in existing_json 
                                      if item.get(key_col) not in {d.get(key_col) for d in data}]
                    combined_json = updated_existing + data
                else:
                    combined_json = data
            except Exception:
                combined_json = data
        else:
            combined_json = data
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(combined_json, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_path}")
        
        return {
            'csv': str(csv_path),
            'json': str(json_path)
        }
    
    def collect_all_ipo_data(self) -> Dict[str, Any]:
        """SEC Edgarì—ì„œ IPO ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
        logger.info("SEC Edgar IPO ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        recent_ipos = []
        
        try:
            # SEC Edgar ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if self.sec_edgar_source.is_available():
                logger.info("SEC Edgarì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                
                # ìµœê·¼ IPO ë°ì´í„° ìˆ˜ì§‘
                recent_data = self.sec_edgar_source.get_recent_ipos(months_back=6)
                if recent_data:
                    recent_ipos.extend(recent_data)
                    logger.info(f"SEC Edgar: ìµœê·¼ IPO {len(recent_data)}ê°œ ìˆ˜ì§‘")
                    
            else:
                logger.warning("SEC Edgar APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"SEC Edgar ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„° ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
        recent_ipos = self._clean_and_deduplicate(recent_ipos)
        
        # íŒŒì¼ ì €ì¥
        recent_files = self._save_to_files(recent_ipos, 'recent_ipos')
        
        logger.info("SEC Edgar IPO ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ")
        
        return {
            'recent_ipos': recent_ipos,
            'files': {
                'recent': recent_files
            },
            'source': 'sec_edgar'
        }

    def get_recent_ipos(self, days_back: int = 365) -> pd.DataFrame:
        """ìµœê·¼ IPO ë°ì´í„°ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ë“¤ì„ ìš°ì„  í™•ì¸
        csv_files = sorted(self.data_dir.glob('recent_ipos_*.csv'))
        if not csv_files:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” íŒŒì¼ì„ í™•ì¸
            recent_ipos_file = self.data_dir / 'recent_ipos.csv'
            if recent_ipos_file.exists():
                csv_files = [recent_ipos_file]
        
        if not csv_files:
            return pd.DataFrame()

        df_list = []
        for file in csv_files:
            from utils.screener_utils import read_csv_flexible
            df = read_csv_flexible(str(file))
            if df is None:
                continue
            if 'ticker' in df.columns and 'symbol' not in df.columns:
                df.rename(columns={'ticker': 'symbol'}, inplace=True)
            if 'date' in df.columns and 'ipo_date' not in df.columns:
                df.rename(columns={'date': 'ipo_date'}, inplace=True)
            if 'price_range' in df.columns and 'ipo_price' not in df.columns:
                df['ipo_price'] = (
                    df['price_range']
                    .astype(str)
                    .str.replace('$', '')
                    .str.split('-')
                    .str[0]
                    .str.replace(',', '')
                )
            df_list.append(df)

        df_all = pd.concat(df_list, ignore_index=True)
        if 'ipo_date' in df_all.columns:
            df_all['ipo_date'] = pd.to_datetime(df_all['ipo_date'], errors='coerce', utc=True)
            cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=days_back)
            df_all = df_all[df_all['ipo_date'] >= cutoff]
        return df_all

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = RealIPODataCollector()
    
    try:
        results = collector.collect_all_ipo_data()
        
        print("\n=== ìˆ˜ì§‘ ê²°ê³¼ ===")
        print(f"ìµœê·¼ IPO ë°ì´í„°: {len(results['recent_ipos'])}ê°œ")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if results['recent_ipos']:
            print("\n=== ìµœê·¼ IPO ìƒ˜í”Œ ===")
            for ipo in results['recent_ipos'][:3]:
                symbol = ipo.get('symbol', 'N/A')
                company = ipo.get('company_name', 'N/A')
                date = ipo.get('ipo_date', 'N/A')
                print(f"- {symbol}: {company} ({date})")
            
    except Exception as e:
        logger.error(f"IPO ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    try:
        collector = RealIPODataCollector()
        results = collector.collect_all_ipo_data()
        
        print(f"\nâœ… IPO ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœê·¼ IPO: {len(results['recent_ipos'])}ê°œ")
        
        # ì €ì¥ëœ íŒŒì¼ ì •ë³´
        print("\n=== ì €ì¥ëœ íŒŒì¼ ===")
        files = results['files']
        if files.get('recent'):
            print(f"- recent_csv: {files['recent']['csv']}")
            print(f"- recent_json: {files['recent']['json']}")
            
    except Exception as e:
        logger.error(f"IPO ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
