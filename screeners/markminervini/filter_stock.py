import pandas as pd
import os
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# configì—ì„œ í•„ìš”í•œ ê²½ë¡œ ì„í¬íŠ¸
from config import (
    RESULTS_DIR, US_WITH_RS_PATH, 
    DATA_DIR, DATA_US_DIR,
    ADVANCED_FINANCIAL_RESULTS_PATH
)

# utilsì—ì„œ í•¨ìˆ˜ import
from utils import ensure_dir, load_csvs_parallel

# í¬ë¦½í†  í•„í„°ë§ í•¨ìˆ˜ ì œê±°ë¨

def filter_us():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_file = os.path.join(RESULTS_DIR, 'us_with_rs.csv')
    
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv(input_file)
    
    # ëª¨ë“  ì¡°ê±´ì´ Trueì¸ ì¢…ëª©ë§Œ í•„í„°ë§
    conditions = [f'cond{i}' for i in range(1, 9)]  # cond1ë¶€í„° cond8ê¹Œì§€
    filtered_df = df[df[conditions].all(axis=1)]
    
    # RS ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_df = filtered_df.sort_values(by='rs_score', ascending=False)
    
    # ê²°ê³¼ë¥¼ ì›ë³¸ íŒŒì¼ì— ë®ì–´ì“°ê¸°
    sorted_df.to_csv(input_file, index=False)
    # JSON íŒŒì¼ ìƒì„± ì¶”ê°€
    json_file = input_file.replace('.csv', '.json')
    sorted_df.to_json(json_file, orient='records', indent=2, force_ascii=False)
    
    # í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
    print(f'[US] ì´ {len(df)}ê°œ ì¤‘ {len(filtered_df)}ê°œ ì¢…ëª©ì´ ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•¨')
    print('[US] ìƒìœ„ 5ê°œ ì¢…ëª©:')
    print(sorted_df[['symbol', 'rs_score']].head(5))

def run_integrated_screening():
    """
    ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ê³¼ ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” í•¨ìˆ˜
    """
    print("\nğŸ” í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹œì‘...")
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ (results/screeners/markminervini í´ë”ì— ì €ì¥)
    from config import MARKMINERVINI_RESULTS_DIR
    INTEGRATED_RESULTS_PATH = os.path.join(MARKMINERVINI_RESULTS_DIR, 'integrated_results.csv')
    
    try:
        # ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ
        if not os.path.exists(US_WITH_RS_PATH):
            print(f"âš ï¸ ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {US_WITH_RS_PATH}")
            return
        
        tech_df = pd.read_csv(US_WITH_RS_PATH)
        print(f"âœ… ê¸°ìˆ ì  ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(tech_df)}ê°œ ì¢…ëª©")
        
        # tech_dfì— symbol ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ì—ì„œ ìƒì„±
        if 'symbol' not in tech_df.columns:
            if tech_df.index.name is None:
                tech_df = tech_df.reset_index()
                tech_df = tech_df.rename(columns={'index': 'symbol'})
            else:
                tech_df = tech_df.reset_index()
        
        # ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ
        if not os.path.exists(ADVANCED_FINANCIAL_RESULTS_PATH):
            print(f"âš ï¸ ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ADVANCED_FINANCIAL_RESULTS_PATH}")
            return
        
        fin_df = pd.read_csv(ADVANCED_FINANCIAL_RESULTS_PATH)
        print(f"âœ… ì¬ë¬´ì œí‘œ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(fin_df)}ê°œ ì¢…ëª©")
        
        # fin_dfì— symbol ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ì—ì„œ ìƒì„±
        if 'symbol' not in fin_df.columns:
            if fin_df.index.name is None:
                fin_df = fin_df.reset_index()
                fin_df = fin_df.rename(columns={'index': 'symbol'})
            else:
                fin_df = fin_df.reset_index()
        
        # ë‘ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì»¬ëŸ¼ ì²˜ë¦¬)
        # 'rs_score' ì»¬ëŸ¼ì´ ì¤‘ë³µë  ê²½ìš° tech_dfì˜ ê²ƒì„ ì‚¬ìš©
        if 'rs_score' in tech_df.columns and 'rs_score' in fin_df.columns:
            fin_df = fin_df.rename(columns={'rs_score': 'rs_score_fin'})
        
        merged_df = pd.merge(tech_df, fin_df, on='symbol', how='inner')
        
        # ì¤‘ë³µ í–‰ ì œê±°
        merged_df = merged_df.drop_duplicates(subset=['symbol'])
        print(f"âœ… í†µí•© ê²°ê³¼: {len(merged_df)}ê°œ ì¢…ëª©")
        
        if merged_df.empty:
            print("âŒ í†µí•© ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê¸°ìˆ ì  ì¡°ê±´ ì¶©ì¡± ìˆ˜ì™€ ì¬ë¬´ì œí‘œ ì¡°ê±´ ì¶©ì¡± ìˆ˜ í•©ì‚°
        if 'met_count' in merged_df.columns and 'fin_met_count' in merged_df.columns:
            merged_df['total_met_count'] = merged_df['met_count'] + merged_df['fin_met_count']
        else:
            print(f"âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(merged_df.columns.tolist())}")
            if 'met_count' not in merged_df.columns:
                print("âŒ 'met_count' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            if 'fin_met_count' not in merged_df.columns:
                print("âŒ 'fin_met_count' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # RS ì ìˆ˜ ì—´ ì²˜ë¦¬
        if 'rs_score_x' in merged_df.columns:
            merged_df['rs_score'] = merged_df['rs_score_x']
            merged_df = merged_df.drop('rs_score_x', axis=1)
        if 'rs_score_y' in merged_df.columns:
            merged_df = merged_df.drop('rs_score_y', axis=1)
        
        # ê²°ê³¼ ì •ë ¬ ë° ì €ì¥ (total_met_count ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ì •ë ¬í•˜ê³ , ê·¸ ë‹¤ìŒ rs_scoreë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
        merged_df = merged_df.sort_values(['total_met_count', 'rs_score'], ascending=[False, False])
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì €ì¥ (symbol, met_count, fin_met_count, total_met_count, rs_score)
        selected_columns = ['symbol', 'met_count', 'fin_met_count', 'total_met_count', 'rs_score']
        filtered_df = merged_df[selected_columns]
        
        ensure_dir(MARKMINERVINI_RESULTS_DIR)
        filtered_df.to_csv(INTEGRATED_RESULTS_PATH, index=False)
        # JSON íŒŒì¼ ìƒì„± ì¶”ê°€
        json_path = INTEGRATED_RESULTS_PATH.replace('.csv', '.json')
        filtered_df.to_json(json_path, orient='records', indent=2, force_ascii=False)
        
        print(f"âœ… í†µí•© ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {INTEGRATED_RESULTS_PATH}")
        print(f"âœ… ì €ì¥ëœ ì»¬ëŸ¼: {', '.join(selected_columns)}")
        
        # ìƒìœ„ 10ê°œ ì¢…ëª© ì¶œë ¥
        top_10 = filtered_df.head(10).reset_index(drop=True)
        print("\nğŸ† í†µí•© ìŠ¤í¬ë¦¬ë‹ ìƒìœ„ 10ê°œ ì¢…ëª©:")
        pd.set_option('display.max_rows', None)
        print(top_10.to_string(index=True))
        
        # í†µí•© ìŠ¤í¬ë¦¬ë„ˆ ì‹¤í–‰ (íŒ¨í„´ ê°ì§€ í¬í•¨)
        try:
            print("\nğŸ” í†µí•© íŒ¨í„´ ê°ì§€ ìŠ¤í¬ë¦¬ë„ˆ ì‹¤í–‰ ì¤‘...")
            from .integrated_screener import run_integrated_screening
            
            # ìƒìœ„ 30ê°œ ì‹¬ë³¼ë§Œ íŒ¨í„´ ê°ì§€
            top_symbols = filtered_df.head(30)['symbol'].tolist()
            if top_symbols:
                pattern_results = run_integrated_screening(max_symbols=len(top_symbols))
                print(f"âœ… íŒ¨í„´ ê°ì§€ ì™„ë£Œ: {len(pattern_results)}ê°œ ì‹¬ë³¼ ì²˜ë¦¬")
            else:
                print("âš ï¸ íŒ¨í„´ ê°ì§€í•  ì‹¬ë³¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ í†µí•© íŒ¨í„´ ê°ì§€ ì˜¤ë¥˜: {e}")
        
    except Exception as e:
        print(f"âŒ í†µí•© ìŠ¤í¬ë¦¬ë‹ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def main():
    parser = argparse.ArgumentParser(description='ì£¼ì‹ ë°ì´í„° í•„í„°ë§ ë° ì •ë ¬ ë„êµ¬')
    parser.add_argument('--integrated', action='store_true', help='í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰ (ê¸°ìˆ ì  + ì¬ë¬´ì œí‘œ)')
    args = parser.parse_args()
    
    if args.integrated:
        # í†µí•© ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰
        run_integrated_screening()
        return
    
    # ê¸°ë³¸ì ìœ¼ë¡œ US ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬
    filter_us()
