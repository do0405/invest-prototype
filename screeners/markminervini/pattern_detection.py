

from __future__ import annotations

import os
import logging
from typing import Dict, List, Tuple, Optional
from datetime import datetime

import pandas as pd
import numpy as np
from scipy.signal import find_peaks, argrelextrema
from scipy.stats import pearsonr

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


# -----------------------------------------------------
# Academic Paper-based Detection Algorithms
# -----------------------------------------------------

def kernel_smoothing(prices: np.ndarray, bandwidth: float = None) -> np.ndarray:
    """ë¹„ëª¨ìˆ˜ ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ê³¡ì„  ìŠ¤ë¬´ë”© (Lo, Mamaysky & Wang 2000)
    
    Args:
        prices: ê°€ê²© ì‹œê³„ì—´ ë°ì´í„°
        bandwidth: ì»¤ë„ ëŒ€ì—­í­ (Noneì´ë©´ ìë™ ê³„ì‚°)
        
    Returns:
        np.ndarray: ìŠ¤ë¬´ë”©ëœ ê°€ê²© ê³¡ì„ 
    """
    n = len(prices)
    if n < 10:
        return prices
    
    # ìµœì  ëŒ€ì—­í­ ê³„ì‚° (CV ìµœì ê°’ì˜ 30% ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •)
    if bandwidth is None:
        # Silverman's rule of thumb ê¸°ë°˜ ëŒ€ì—­í­
        std_prices = np.std(prices)
        bandwidth = 1.06 * std_prices * (n ** (-1/5)) * 0.3
    
    smoothed = np.zeros_like(prices)
    x_points = np.arange(n)
    
    for i in range(n):
        # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = np.exp(-0.5 * ((x_points - i) / bandwidth) ** 2)
        weights /= np.sum(weights)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìŠ¤ë¬´ë”© ê°’ ê³„ì‚°
        smoothed[i] = np.sum(weights * prices)
    
    return smoothed


def extract_peaks_troughs(smoothed_prices: np.ndarray, min_distance: int = 5) -> Tuple[np.ndarray, np.ndarray]:
    """ìŠ¤ë¬´ë”©ëœ ê³¡ì„ ì—ì„œ í”¼í¬ì™€ ê³¨ ì¶”ì¶œ
    
    Args:
        smoothed_prices: ìŠ¤ë¬´ë”©ëœ ê°€ê²© ë°ì´í„°
        min_distance: í”¼í¬ ê°„ ìµœì†Œ ê±°ë¦¬
        
    Returns:
        Tuple[np.ndarray, np.ndarray]: (í”¼í¬ ì¸ë±ìŠ¤, ê³¨ ì¸ë±ìŠ¤)
    """
    # í”¼í¬ ì°¾ê¸°
    peaks, _ = find_peaks(smoothed_prices, distance=min_distance)
    
    # ê³¨ ì°¾ê¸° (ìŒìˆ˜ë¡œ ë³€í™˜ í›„ í”¼í¬ ì°¾ê¸°)
    troughs, _ = find_peaks(-smoothed_prices, distance=min_distance)
    
    return peaks, troughs


def calculate_amplitude_contraction(peaks: np.ndarray, troughs: np.ndarray, prices: np.ndarray) -> List[float]:
    """ì—°ì† í”¼í¬ ê°„ ì§„í­ ìˆ˜ì¶• ê³„ì‚° (Suh, Li & Gao 2008)
    
    Args:
        peaks: í”¼í¬ ì¸ë±ìŠ¤ ë°°ì—´
        troughs: ê³¨ ì¸ë±ìŠ¤ ë°°ì—´  
        prices: ê°€ê²© ë°ì´í„°
        
    Returns:
        List[float]: ê° êµ¬ê°„ì˜ ì§„í­ ë¹„ìœ¨
    """
    if len(peaks) < 2:
        return []
    
    amplitudes = []
    
    for i in range(len(peaks) - 1):
        peak1_idx = peaks[i]
        peak2_idx = peaks[i + 1]
        
        # ë‘ í”¼í¬ ì‚¬ì´ì˜ ìµœì €ì  ì°¾ê¸°
        between_troughs = troughs[(troughs > peak1_idx) & (troughs < peak2_idx)]
        if len(between_troughs) > 0:
            trough_idx = between_troughs[np.argmin(prices[between_troughs])]
            
            # ì§„í­ ê³„ì‚° (í”¼í¬ì—ì„œ ê³¨ê¹Œì§€ì˜ ìµœëŒ€ í•˜ë½í­)
            amplitude = max(
                prices[peak1_idx] - prices[trough_idx],
                prices[peak2_idx] - prices[trough_idx]
            )
            amplitudes.append(amplitude)
    
    return amplitudes


def quadratic_fit_cup(cup_indices: np.ndarray, prices: np.ndarray) -> Tuple[float, float]:
    """2ì°¨ ë‹¤í•­ì‹ ê·¼ì‚¬ë¥¼ ì´ìš©í•œ Uìí˜• ì»µ ê²€ì¦ (Suh, Li & Gao 2008)
    
    Args:
        cup_indices: ì»µ êµ¬ê°„ì˜ ì¸ë±ìŠ¤
        prices: í•´ë‹¹ êµ¬ê°„ì˜ ê°€ê²© ë°ì´í„°
        
    Returns:
        Tuple[float, float]: (R-squared, ê³¡ë¥ )
    """
    if len(cup_indices) < 3:
        return 0.0, 0.0
    
    try:
        # 2ì°¨ ë‹¤í•­ì‹ í”¼íŒ…: f(t) = at^2 + bt + c
        coeffs = np.polyfit(cup_indices, prices, 2)
        fitted_prices = np.polyval(coeffs, cup_indices)
        
        # R-squared ê³„ì‚°
        ss_res = np.sum((prices - fitted_prices) ** 2)
        ss_tot = np.sum((prices - np.mean(prices)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        # ê³¡ë¥  ê³„ì‚° (2ì°¨ ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’)
        curvature = abs(coeffs[0])
        
        return r_squared, curvature
    except:
        return 0.0, 0.0


def bezier_curve_correlation(control_points: np.ndarray, actual_prices: np.ndarray) -> float:
    """ë² ì§€ì–´ ê³¡ì„ ê³¼ ì‹¤ì œ ê°€ê²©ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (Suh, Li & Gao 2008)
    
    Args:
        control_points: ë² ì§€ì–´ ê³¡ì„  ì œì–´ì 
        actual_prices: ì‹¤ì œ ê°€ê²© ë°ì´í„°
        
    Returns:
        float: í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
    """
    if len(control_points) < 3 or len(actual_prices) < 3:
        return 0.0
    
    try:
        # ê°„ë‹¨í•œ ë² ì§€ì–´ ê³¡ì„  ê·¼ì‚¬ (3ì°¨ ë‹¤í•­ì‹ ì‚¬ìš©)
        t = np.linspace(0, 1, len(actual_prices))
        
        # ì œì–´ì ì„ ì´ìš©í•œ ë² ì§€ì–´ ê³¡ì„  ìƒì„±
        if len(control_points) >= 4:
            # 3ì°¨ ë² ì§€ì–´ ê³¡ì„ 
            bezier_curve = (
                (1-t)**3 * control_points[0] +
                3*(1-t)**2*t * control_points[1] +
                3*(1-t)*t**2 * control_points[2] +
                t**3 * control_points[3]
            )
        else:
            # 2ì°¨ ë² ì§€ì–´ ê³¡ì„ 
            bezier_curve = (
                (1-t)**2 * control_points[0] +
                2*(1-t)*t * control_points[1] +
                t**2 * control_points[2]
            )
        
        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation, _ = pearsonr(bezier_curve, actual_prices)
        return correlation if not np.isnan(correlation) else 0.0
    except:
        return 0.0


def detect_vcp(df: pd.DataFrame) -> bool:
    """í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ VCP íŒ¨í„´ ê°ì§€ (Lo, Mamaysky & Wang 2000; Suh, Li & Gao 2008)
    
    Args:
        df: ì£¼ê°€ ë°ì´í„° (ìµœì†Œ 60ì¼ ì´ìƒì˜ ë°ì´í„° í•„ìš”)
        
    Returns:
        bool: VCP íŒ¨í„´ ê°ì§€ ì—¬ë¶€
    """
    if df is None or len(df) < 60:
        return False

    # ìµœê·¼ 90ì¼ ë°ì´í„° ì‚¬ìš©
    recent = df.tail(90).copy()
    prices = recent["close"].values
    volumes = recent["volume"].values
    
    # 1. ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ê³¡ì„  ìŠ¤ë¬´ë”©
    smoothed_prices = kernel_smoothing(prices)
    
    # 2. ìŠ¤ë¬´ë”©ëœ ê³¡ì„ ì—ì„œ í”¼í¬ì™€ ê³¨ ì¶”ì¶œ
    peaks, troughs = extract_peaks_troughs(smoothed_prices)
    
    if len(peaks) < 3:  # ìµœì†Œ 3ê°œì˜ í”¼í¬ í•„ìš”
        return False
    
    # 3. ì—°ì†ì  ë³€ë™ì„± ìˆ˜ì¶• ê²€ì¶œ
    amplitudes = calculate_amplitude_contraction(peaks, troughs, smoothed_prices)
    
    if len(amplitudes) < 2:  # ìµœì†Œ 2íšŒ ìˆ˜ì¶• í•„ìš”
        return False
    
    # 4. ì§„í­ ê°ì†Œ íŒ¨í„´ í™•ì¸
    contraction_count = 0
    for i in range(1, len(amplitudes)):
        if amplitudes[i] < amplitudes[i-1] * 0.85:  # 15% ì´ìƒ ê°ì†Œ
            contraction_count += 1
    
    if contraction_count < 2:  # ìµœì†Œ 2íšŒ ì—°ì† ìˆ˜ì¶•
        return False
    
    # 5. ê±°ë˜ëŸ‰ íŒ¨í„´ í™•ì¸ (ìˆ˜ì¶• ì‹œ ê±°ë˜ëŸ‰ ê°ì†Œ)
    volume_ma = pd.Series(volumes).rolling(10).mean().values
    recent_volume_trend = volume_ma[-10:]
    
    if len(recent_volume_trend) > 5:
        volume_decrease = recent_volume_trend[-1] < recent_volume_trend[0] * 0.8
        if not volume_decrease:
            return False
    
    # 6. ìµœì¢… ë¸Œë ˆì´í¬ì•„ì›ƒ í™•ì¸
    last_peak_price = smoothed_prices[peaks[-1]]
    current_price = prices[-1]
    
    # í˜„ì¬ ê°€ê²©ì´ ë§ˆì§€ë§‰ í”¼í¬ ê·¼ì²˜ì— ìˆì–´ì•¼ í•¨
    if current_price < last_peak_price * 0.95:
        return False
    
    return True


def detect_cup_and_handle(df: pd.DataFrame, window: int = 180) -> bool:
    """í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ Cup-with-Handle íŒ¨í„´ ê°ì§€ (Suh, Li & Gao 2008)
    
    Args:
        df: ì£¼ê°€ ë°ì´í„° (ìµœì†Œ windowì¼ ì´ìƒì˜ ë°ì´í„° í•„ìš”)
        window: ë¶„ì„í•  ê¸°ê°„ (ê¸°ë³¸ê°’: 180ì¼)
        
    Returns:
        bool: Cup-with-Handle íŒ¨í„´ ê°ì§€ ì—¬ë¶€
    """
    if df is None or len(df) < window:
        return False

    data = df.tail(window).copy()
    prices = data["close"].values
    volumes = data["volume"].values
    
    # 1. ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ê³¡ì„  ìŠ¤ë¬´ë”©
    smoothed_prices = kernel_smoothing(prices)
    
    # 2. í”¼í¬ì™€ ê³¨ ì¶”ì¶œ
    peaks, troughs = extract_peaks_troughs(smoothed_prices)
    
    if len(peaks) < 2 or len(troughs) == 0:
        return False

    # 3. ì»µ êµ¬ì¡° ì‹ë³„ (ì¢Œì¸¡ ê³ ì  - ë°”ë‹¥ - ìš°ì¸¡ ê³ ì )
    left_peak = peaks[0]
    right_candidates = peaks[peaks > left_peak]
    if len(right_candidates) == 0:
        return False
    
    right_peak = right_candidates[-1]
    bottom_candidates = troughs[(troughs > left_peak) & (troughs < right_peak)]
    if len(bottom_candidates) == 0:
        return False
    
    bottom = bottom_candidates[np.argmin(smoothed_prices[bottom_candidates])]

    # 4. ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
    if right_peak - left_peak < 30:  # ìµœì†Œ 30ì¼ ì»µ í˜•ì„± ê¸°ê°„
        return False
    
    if bottom - left_peak < 8 or right_peak - bottom < 8:  # ì¢Œìš° ê· í˜•
        return False

    # 5. 2ì°¨ ë‹¤í•­ì‹ ê·¼ì‚¬ë¥¼ ì´ìš©í•œ Uìí˜• ì»µ ê²€ì¦
    cup_indices = np.arange(left_peak, right_peak + 1)
    cup_prices = smoothed_prices[left_peak:right_peak + 1]
    
    r_squared, curvature = quadratic_fit_cup(cup_indices, cup_prices)
    
    if r_squared < 0.7:  # R-squared ì„ê³„ê°’
        return False
    
    if curvature < 0.0001:  # ì¶©ë¶„í•œ ê³¡ë¥  í•„ìš”
        return False

    # 6. ë² ì§€ì–´ ê³¡ì„  ìƒê´€ê³„ìˆ˜ ê²€ì¦
    # 7ê°œ ì œì–´ì  ì„ ì •: ì¢Œì¸¡ ê³ ì , ì¤‘ê°„ì ë“¤, ë°”ë‹¥, ìš°ì¸¡ ê³ ì 
    control_points = np.array([
        smoothed_prices[left_peak],
        smoothed_prices[left_peak + (bottom - left_peak) // 2],
        smoothed_prices[bottom],
        smoothed_prices[bottom + (right_peak - bottom) // 2],
        smoothed_prices[right_peak]
    ])
    
    correlation = bezier_curve_correlation(control_points, cup_prices)
    
    if correlation < 0.85:  # ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì„ê³„ê°’
        return False

    # 7. ì¢Œìš° ê³ ì  ëŒ€ì¹­ì„± ê²€ì¦
    left_high = smoothed_prices[left_peak]
    right_high = smoothed_prices[right_peak]
    height_diff = abs(left_high - right_high) / min(left_high, right_high) * 100
    
    if height_diff > 5:  # 5% ì´ë‚´ ì°¨ì´
        return False

    # 8. ì»µ ê¹Šì´ ê²€ì¦
    bottom_low = smoothed_prices[bottom]
    depth = (min(left_high, right_high) - bottom_low) / min(left_high, right_high) * 100
    
    if not (12 <= depth <= 50):  # ì ì ˆí•œ ê¹Šì´
        return False

    # 9. í•¸ë“¤ ê²€ì¦
    handle_start = right_peak
    handle_prices = smoothed_prices[handle_start:]
    
    if len(handle_prices) < 5:  # ìµœì†Œ í•¸ë“¤ ê¸¸ì´
        return False
    
    handle_low = np.min(handle_prices)
    handle_depth = (right_high - handle_low) / right_high * 100
    
    # í•¸ë“¤ ê¹Šì´ê°€ ì»µ ê¹Šì´ì˜ 33% ì´ë‚´ (ë…¼ë¬¸ ê¸°ì¤€)
    if handle_depth > depth * 0.33:
        return False
    
    if handle_depth < 2 or handle_depth > 25:  # ì ì ˆí•œ í•¸ë“¤ ê¹Šì´
        return False

    # 10. ê±°ë˜ëŸ‰ íŒ¨í„´ ê²€ì¦
    avg_volume = pd.Series(volumes).rolling(20).mean().values
    
    # ì»µ ë°”ë‹¥ì—ì„œ ê±°ë˜ëŸ‰ ê°ì†Œ
    bottom_vol = volumes[max(0, bottom - 2): bottom + 3].mean()
    if bottom_vol >= avg_volume[bottom] * 0.7:
        return False
    
    # í•¸ë“¤ êµ¬ê°„ ê±°ë˜ëŸ‰ í™•ì¸
    handle_vol = volumes[handle_start:].mean() if len(volumes[handle_start:]) > 0 else volumes[handle_start]
    if handle_vol >= bottom_vol * 1.2:
        return False
    
    # ìµœê·¼ ë¸Œë ˆì´í¬ì•„ì›ƒ ê±°ë˜ëŸ‰
    if len(volumes) > 0 and len(avg_volume) > 0:
        if volumes[-1] < avg_volume[-1] * 1.2:
            return False

    # 11. ì»µ í˜•ì„± ì¤‘ ê±°ë˜ëŸ‰ ê°ì†Œ íŠ¸ë Œë“œ
    cup_volumes = volumes[left_peak:right_peak]
    if len(cup_volumes) > 10:
        early_vol = cup_volumes[:len(cup_volumes)//3].mean()
        late_vol = cup_volumes[-len(cup_volumes)//3:].mean()
        if late_vol >= early_vol * 1.1:  # í›„ë°˜ë¶€ ê±°ë˜ëŸ‰ì´ ë„ˆë¬´ ì¦ê°€í•˜ë©´ ì•ˆë¨
            return False

    return True


# -----------------------------------------------------
# Batch analysis
# -----------------------------------------------------

from config import MARKMINERVINI_RESULTS_DIR


def analyze_tickers_from_results(results_dir: str, data_dir: str, output_dir: str = MARKMINERVINI_RESULTS_DIR) -> pd.DataFrame:
    """CSV íŒŒì¼ì—ì„œ í‹°ì»¤ ëª©ë¡ì„ ì½ê³  íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        results_dir: ì¬ë¬´ ê²°ê³¼ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        data_dir: ì£¼ê°€ ë°ì´í„° CSV íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_dir: ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        pd.DataFrame: íŒ¨í„´ ê°ì§€ ê²°ê³¼
        
    Raises:
        FileNotFoundError: ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°
    """
    os.makedirs(output_dir, exist_ok=True)
    results_file = os.path.join(results_dir, "advanced_financial_results.csv")
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file}")

    logger.info(f"ì¬ë¬´ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘: {results_file}")
    results_df = pd.read_csv(results_file)
    logger.info(f"ì´ {len(results_df)}ê°œ ì¢…ëª©ì— ëŒ€í•œ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
    
    analysis = []

    for _, row in results_df.iterrows():
        symbol = row['symbol']
        fin_met_count = row.get('fin_met_count', 0)
        # fin_met_count ì¡°ê±´ ì œê±° - ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
        file_path = os.path.join(data_dir, f"{symbol}.csv")
        if not os.path.exists(file_path):
            continue
        df = pd.read_csv(file_path)
        date_col = next((c for c in df.columns if c.lower() in ['date', 'ë‚ ì§œ', 'ì¼ì']), None)
        if not date_col:
            continue
        df[date_col] = pd.to_datetime(df[date_col], utc=True)
        df.set_index(date_col, inplace=True)
        col_map = {'high': ['high', 'High', 'ê³ ê°€'], 'low': ['low', 'Low', 'ì €ê°€'], 'close': ['close', 'Close', 'ì¢…ê°€'], 'volume': ['volume', 'Volume', 'ê±°ë˜ëŸ‰']}
        found = {}
        for k, names in col_map.items():
            for c in df.columns:
                if c.lower() in [n.lower() for n in names]:
                    found[k] = c
                    break
        if len(found) < 4:
            continue
        df = df.rename(columns={v: k for k, v in found.items()})

        vcp = detect_vcp(df)
        cup = detect_cup_and_handle(df)
        if not vcp and not cup:
            continue

        analysis.append({
            'symbol': symbol,
            'fin_met_count': fin_met_count,
            'vcp': vcp,
            'cup_handle': cup,
        })

    if not analysis:
        return pd.DataFrame()

    out_df = pd.DataFrame(analysis)
    out_df = out_df.sort_values(['vcp', 'cup_handle'], ascending=False)
    out_file = os.path.join(output_dir, 'pattern_analysis_results.csv')
    out_df.to_csv(out_file, index=False, encoding='utf-8-sig')
    out_df.to_json(out_file.replace('.csv', '.json'), orient='records', indent=2, force_ascii=False)
    return out_df


def run_pattern_detection_on_financial_results() -> Optional[pd.DataFrame]:
    """advanced_financial_results.csvì˜ í‹°ì»¤ë“¤ì— ëŒ€í•´ íŒ¨í„´ ê°ì§€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥
    
    Returns:
        Optional[pd.DataFrame]: íŒ¨í„´ ê°ì§€ ê²°ê³¼ DataFrame ë˜ëŠ” ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° None
    """
    import sys
    import os
    from datetime import datetime
    
    # ê²½ë¡œ ì„¤ì • ìµœì í™”
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    from config import ADVANCED_FINANCIAL_RESULTS_PATH, DATA_US_DIR
    from utils.io_utils import process_stock_data
    
    start_time = datetime.now()
    
    # advanced_financial_results.csv ì½ê¸°
    if not os.path.exists(ADVANCED_FINANCIAL_RESULTS_PATH):
        logger.error(f"âŒ {ADVANCED_FINANCIAL_RESULTS_PATH} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    try:
        financial_df = pd.read_csv(ADVANCED_FINANCIAL_RESULTS_PATH)
        if financial_df.empty:
            logger.warning("âŒ advanced_financial_results.csvê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None
    
    logger.info(f"ğŸ“Š {len(financial_df)} ê°œ ì¢…ëª©ì— ëŒ€í•´ íŒ¨í„´ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“Š {len(financial_df)} ê°œ ì¢…ëª©ì— ëŒ€í•´ íŒ¨í„´ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    pattern_results = []
    processed_count = 0
    pattern_count = 0
    
    for idx, row in financial_df.iterrows():
        symbol = row['symbol']
        fin_met_count = row.get('fin_met_count', 0)
        processed_count += 1
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ (10% ë‹¨ìœ„ë¡œ)
        if processed_count % max(1, len(financial_df) // 10) == 0:
            progress = processed_count / len(financial_df) * 100
            logger.info(f"ì§„í–‰ ì¤‘: {progress:.1f}% ì™„ë£Œ ({processed_count}/{len(financial_df)})")
        
        try:
            # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì£¼ê°€ ë°ì´í„° ì½ê¸°
            csv_file = f"{symbol}.csv"
            _, stock_data_full, _ = process_stock_data(csv_file, DATA_US_DIR, min_days=60, recent_days=365)
            
            if stock_data_full is None or len(stock_data_full) < 60:
                logger.debug(f"âš ï¸ {symbol}: ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ìµœê·¼ 1ë…„ ë°ì´í„°ë§Œ ì‚¬ìš©
            stock_data = stock_data_full.tail(365).copy()
            
            # date ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            if 'date' in stock_data.columns and not isinstance(stock_data.index, pd.DatetimeIndex):
                stock_data = stock_data.set_index('date')
            
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            
            # ì»¬ëŸ¼ëª… ë§¤í•‘ - ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬
            col_mapping = {}
            for req_col in required_cols:
                for col in stock_data.columns:
                    if req_col.lower() == col.lower() or req_col.lower() in col.lower():
                        col_mapping[col] = req_col
                        break
            
            if len(col_mapping) < len(required_cols):
                missing_cols = set(required_cols) - set(col_mapping.values())
                logger.debug(f"âš ï¸ {symbol}: í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ëˆ„ë½: {missing_cols}")
                continue
            
            # ì»¬ëŸ¼ëª… ë³€ê²½
            stock_data = stock_data.rename(columns=col_mapping)
            
            # íŒ¨í„´ ê°ì§€
            vcp_detected = detect_vcp(stock_data)
            cup_detected = detect_cup_and_handle(stock_data)
            
            # í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ ê²°ê³¼ì— ì¶”ê°€
            if vcp_detected or cup_detected:
                pattern_count += 1
                pattern_results.append({
                    'symbol': symbol,
                    'fin_met_count': fin_met_count,
                    'rs_score': row.get('rs_score', None),
                    'rs_percentile': row.get('rs_percentile', None),
                    'fin_percentile': row.get('fin_percentile', None),
                    'total_percentile': row.get('total_percentile', None),
                    'vcp_pattern': vcp_detected,
                    'cup_handle_pattern': cup_detected,
                    'has_error': row.get('has_error', False),
                    'detection_date': datetime.now().strftime('%Y-%m-%d')
                })
                logger.info(f"âœ… {symbol}: VCP={vcp_detected}, Cup&Handle={cup_detected}")
                print(f"âœ… {symbol}: VCP={vcp_detected}, Cup&Handle={cup_detected}")
            
        except Exception as e:
            logger.error(f"âš ï¸ {symbol} íŒ¨í„´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    elapsed_time = datetime.now() - start_time
    
    # ê²°ê³¼ ì €ì¥
    if pattern_results:
        results_df = pd.DataFrame(pattern_results)
        
        # ì •ë ¬: VCPì™€ Cup&Handle íŒ¨í„´ ìš°ì„ , ê·¸ ë‹¤ìŒ total_percentile
        results_df['pattern_score'] = results_df['vcp_pattern'].astype(int) + results_df['cup_handle_pattern'].astype(int)
        results_df = results_df.sort_values(['pattern_score', 'total_percentile'], ascending=[False, False])
        results_df = results_df.drop('pattern_score', axis=1)
        
        # markminervini í´ë”ì— ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” íŒŒì¼ë§Œ ìƒì„±)
        output_dir = os.path.dirname(ADVANCED_FINANCIAL_RESULTS_PATH)
        csv_path = os.path.join(output_dir, 'pattern_detection_results.csv')
        json_path = os.path.join(output_dir, 'pattern_detection_results.json')
        
        try:
            results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            results_df.to_json(json_path, orient='records', indent=2, force_ascii=False)
            
            logger.info(f"\nğŸ¯ íŒ¨í„´ ê°ì§€ ì™„ë£Œ: {len(results_df)}ê°œ ì¢…ëª©ì´ íŒ¨í„´ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
            logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥: {csv_path}")
            logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥: {json_path}")
            
            print(f"\nğŸ¯ íŒ¨í„´ ê°ì§€ ì™„ë£Œ: {len(results_df)}ê°œ ì¢…ëª©ì´ íŒ¨í„´ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {csv_path}")
            
            # ìƒìœ„ 10ê°œ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ† ìƒìœ„ 10ê°œ íŒ¨í„´ ê°ì§€ ê²°ê³¼:")
            top_10 = results_df.head(10)
            print(top_10[['symbol', 'fin_met_count', 'vcp_pattern', 'cup_handle_pattern', 'total_percentile']])
            
            # ì‹¤í–‰ í†µê³„ ì¶œë ¥
            print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed_time}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜: {processed_count}")
            print(f"âœ… íŒ¨í„´ ê°ì§€ëœ ì¢…ëª© ìˆ˜: {pattern_count}")
            print(f"ğŸ“ˆ íŒ¨í„´ ê°ì§€ ë¹„ìœ¨: {pattern_count/processed_count*100:.2f}%")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return results_df
    else:
        logger.warning("âŒ íŒ¨í„´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("âŒ íŒ¨í„´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¹ˆ DataFrameì´ë¼ë„ ì»¬ëŸ¼ í—¤ë”ì™€ í•¨ê»˜ íŒŒì¼ ìƒì„±
        empty_df = pd.DataFrame(columns=[
            'symbol', 'fin_met_count', 'rs_score', 'rs_percentile', 
            'fin_percentile', 'total_percentile', 'vcp_pattern', 
            'cup_handle_pattern', 'has_error', 'detection_date'
        ])
        
        # markminervini í´ë”ì— ë¹ˆ íŒŒì¼ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” íŒŒì¼ë§Œ ìƒì„±)
        output_dir = os.path.dirname(ADVANCED_FINANCIAL_RESULTS_PATH)
        csv_path = os.path.join(output_dir, 'pattern_detection_results.csv')
        json_path = os.path.join(output_dir, 'pattern_detection_results.json')
        
        try:
            empty_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            empty_df.to_json(json_path, orient='records', indent=2, force_ascii=False)
            
            print(f"ğŸ“ ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„±: {csv_path}")
            
            # ì‹¤í–‰ í†µê³„ ì¶œë ¥
            print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed_time}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜: {processed_count}")
            print(f"âœ… íŒ¨í„´ ê°ì§€ëœ ì¢…ëª© ìˆ˜: 0")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return empty_df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        run_pattern_detection_on_financial_results()
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    return 0


if __name__ == "__main__":
    main()