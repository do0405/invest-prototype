"""í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ VCP ë° Cup-with-Handle íŒ¨í„´ ê°ì§€ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ Lo, Mamaysky & Wang (2000)ê³¼ Suh, Li & Gao (2008) ë…¼ë¬¸ì˜
ë°©ë²•ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ê¸‰ íŒ¨í„´ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë¹„ëª¨ìˆ˜ ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ê³¡ì„  ìŠ¤ë¬´ë”©
- ì—°ì†ì  ë³€ë™ì„± ìˆ˜ì¶• ê²€ì¶œ (VCP)
- 2ì°¨ ë‹¤í•­ì‹ ê·¼ì‚¬ë¥¼ ì´ìš©í•œ Uìí˜• ì»µ ê²€ì¦
- ë² ì§€ì–´ ê³¡ì„ ê³¼ ìƒê´€ê³„ìˆ˜ ë¹„êµ (Cup & Handle)
- ë°°ì¹˜ ì²˜ë¦¬ ë° CSV/JSON ê²°ê³¼ ì¶œë ¥
"""

from __future__ import annotations

import os
import sys
import logging
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
from scipy.signal import find_peaks, argrelextrema
from scipy import stats
from scipy.optimize import curve_fit
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import cross_val_score

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from config import MARKMINERVINI_RESULTS_DIR, ADVANCED_FINANCIAL_RESULTS_PATH, DATA_US_DIR
from utils.io_utils import process_stock_data

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class KernelSmoothing:
    """ë¹„ëª¨ìˆ˜ ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ê³¡ì„  ìŠ¤ë¬´ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, bandwidth: Optional[float] = None):
        self.bandwidth = bandwidth
        self.smoothed_prices = None
        self.original_prices = None
    
    def gaussian_kernel(self, u: np.ndarray, h: float) -> np.ndarray:
        """ê°€ìš°ì‹œì•ˆ ì»¤ë„ í•¨ìˆ˜"""
        return (1 / (h * np.sqrt(2 * np.pi))) * np.exp(-u**2 / (2 * h**2))
    
    def fit_smooth(self, prices: np.ndarray, dates: Optional[np.ndarray] = None) -> np.ndarray:
        """ì»¤ë„ íšŒê·€ë¥¼ ì´ìš©í•œ ê°€ê²© ìŠ¤ë¬´ë”©"""
        self.original_prices = prices
        n = len(prices)
        
        if dates is None:
            x = np.arange(n)
        else:
            x = np.arange(n)
        
        # ìµœì  ëŒ€ì—­í­ ê²°ì • (êµì°¨ê²€ì¦ë²•)
        if self.bandwidth is None:
            # ì‹¤ë¬´ì  ì¡°ì •: CV ìµœì ê°’ì˜ 30% ìˆ˜ì¤€
            h_cv = self._cross_validation_bandwidth(x, prices)
            self.bandwidth = h_cv * 0.3
        
        smoothed = np.zeros(n)
        
        for i in range(n):
            weights = self.gaussian_kernel(x - x[i], self.bandwidth)
            smoothed[i] = np.sum(weights * prices) / np.sum(weights)
        
        self.smoothed_prices = smoothed
        return smoothed
    
    def _cross_validation_bandwidth(self, x: np.ndarray, y: np.ndarray) -> float:
        """êµì°¨ê²€ì¦ì„ í†µí•œ ìµœì  ëŒ€ì—­í­ ê²°ì •"""
        bandwidths = np.logspace(-2, 1, 20)  # 0.01 to 10
        best_score = -np.inf
        best_h = bandwidths[0]
        
        for h in bandwidths:
            scores = []
            for i in range(len(x)):
                # Leave-one-out cross validation
                x_train = np.delete(x, i)
                y_train = np.delete(y, i)
                x_test = x[i]
                y_test = y[i]
                
                weights = self.gaussian_kernel(x_train - x_test, h)
                if np.sum(weights) > 0:
                    y_pred = np.sum(weights * y_train) / np.sum(weights)
                    scores.append(-(y_test - y_pred)**2)  # Negative MSE
            
            avg_score = np.mean(scores)
            if avg_score > best_score:
                best_score = avg_score
                best_h = h
        
        return best_h
    
    def find_peaks_troughs(self) -> Tuple[np.ndarray, np.ndarray]:
        """ìŠ¤ë¬´ë”©ëœ ê³¡ì„ ì—ì„œ í”¼í¬ì™€ ê³¨ ì¶”ì¶œ"""
        if self.smoothed_prices is None:
            raise ValueError("ë¨¼ì € fit_smooth()ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # í”¼í¬ ì°¾ê¸°
        peaks, _ = find_peaks(self.smoothed_prices, distance=5)
        
        # ê³¨ ì°¾ê¸° (ìŒìˆ˜ë¡œ ë³€í™˜ í›„ í”¼í¬ ì°¾ê¸°)
        troughs, _ = find_peaks(-self.smoothed_prices, distance=5)
        
        return peaks, troughs


class VCPDetector:
    """Volatility Contraction Pattern ê°ì§€ê¸°"""
    
    def __init__(self, min_contractions: int = 2):
        self.min_contractions = min_contractions
        self.smoother = KernelSmoothing()
    
    def detect(self, df: pd.DataFrame) -> bool:
        """VCP íŒ¨í„´ ê°ì§€
        
        Args:
            df: ì£¼ê°€ ë°ì´í„° (ìµœì†Œ 60ì¼ ì´ìƒ)
            
        Returns:
            bool: VCP íŒ¨í„´ ê°ì§€ ì—¬ë¶€
        """
        if df is None or len(df) < 60:
            return False
        
        # ìµœê·¼ 90ì¼ ë°ì´í„° ì‚¬ìš©
        recent = df.tail(90).copy()
        prices = recent['close'].values
        
        # ì»¤ë„ ìŠ¤ë¬´ë”© ì ìš©
        smoothed = self.smoother.fit_smooth(prices)
        peaks, troughs = self.smoother.find_peaks_troughs()
        
        if len(peaks) < 3:  # ìµœì†Œ 3ê°œ í”¼í¬ í•„ìš”
            return False
        
        # ì—°ì†ì  ì§„í­ ê°ì†Œ ê²€ì¦
        amplitudes = []
        for i in range(len(peaks) - 1):
            peak_idx = peaks[i]
            next_peak_idx = peaks[i + 1]
            
            # ë‘ í”¼í¬ ì‚¬ì´ì˜ ìµœì €ì  ì°¾ê¸°
            trough_candidates = troughs[(troughs > peak_idx) & (troughs < next_peak_idx)]
            if len(trough_candidates) == 0:
                continue
            
            trough_idx = trough_candidates[np.argmin(smoothed[trough_candidates])]
            
            # ì§„í­ ê³„ì‚°
            amplitude = smoothed[peak_idx] - smoothed[trough_idx]
            amplitudes.append(amplitude)
        
        # ì—°ì†ì  ê°ì†Œ í™•ì¸
        contractions = 0
        for i in range(1, len(amplitudes)):
            if amplitudes[i] < amplitudes[i-1]:
                contractions += 1
            else:
                contractions = 0  # ì—°ì†ì„± ê¹¨ì§
        
        return contractions >= self.min_contractions


class CupHandleDetector:
    """Cup & Handle íŒ¨í„´ ê°ì§€ê¸°"""
    
    def __init__(self, correlation_threshold: float = 0.85):
        self.correlation_threshold = correlation_threshold
        self.smoother = KernelSmoothing()
    
    def detect(self, df: pd.DataFrame, window: int = 180) -> bool:
        """Cup & Handle íŒ¨í„´ ê°ì§€
        
        Args:
            df: ì£¼ê°€ ë°ì´í„°
            window: ë¶„ì„ ê¸°ê°„
            
        Returns:
            bool: Cup & Handle íŒ¨í„´ ê°ì§€ ì—¬ë¶€
        """
        if df is None or len(df) < window:
            return False
        
        data = df.tail(window).copy()
        prices = data['close'].values
        
        # ì»¤ë„ ìŠ¤ë¬´ë”© ì ìš©
        smoothed = self.smoother.fit_smooth(prices)
        peaks, troughs = self.smoother.find_peaks_troughs()
        
        if len(peaks) < 2 or len(troughs) == 0:
            return False
        
        # ì»µ êµ¬ê°„ ì‹ë³„
        left_peak = peaks[0]
        right_candidates = peaks[peaks > left_peak]
        if len(right_candidates) == 0:
            return False
        
        right_peak = right_candidates[-1]
        bottom_candidates = troughs[(troughs > left_peak) & (troughs < right_peak)]
        if len(bottom_candidates) == 0:
            return False
        
        bottom = bottom_candidates[np.argmin(smoothed[bottom_candidates])]
        
        # ì»µ í˜•ì„± ê¸°ê°„ ê²€ì¦ (ìµœì†Œ 30ì¼)
        if right_peak - left_peak < 30:
            return False
        
        # Uìí˜• ê²€ì¦ (2ì°¨ ë‹¤í•­ì‹ ê·¼ì‚¬)
        if not self._verify_u_shape(smoothed, left_peak, bottom, right_peak):
            return False
        
        # ë² ì§€ì–´ ê³¡ì„  ìƒê´€ê³„ìˆ˜ ê²€ì¦
        if not self._verify_bezier_correlation(smoothed, left_peak, bottom, right_peak):
            return False
        
        # í•¸ë“¤ ê²€ì¦
        if not self._verify_handle(smoothed, right_peak, len(smoothed) - 1):
            return False
        
        return True
    
    def _verify_u_shape(self, smoothed: np.ndarray, left: int, bottom: int, right: int) -> bool:
        """2ì°¨ ë‹¤í•­ì‹ì„ ì´ìš©í•œ Uìí˜• ê²€ì¦"""
        try:
            # ì»µ êµ¬ê°„ ë°ì´í„°
            x_cup = np.array([left, bottom, right])
            y_cup = smoothed[x_cup]
            
            # 2ì°¨ í•¨ìˆ˜ í”¼íŒ…: f(t) = at^2 + bt + c
            def quadratic(x, a, b, c):
                return a * x**2 + b * x + c
            
            popt, _ = curve_fit(quadratic, x_cup, y_cup)
            a, b, c = popt
            
            # ê³¡ë¥  ê²€ì¦ (a > 0ì´ë©´ ì•„ë˜ë¡œ ë³¼ë¡)
            if a <= 0:
                return False
            
            # ëŒ€ì¹­ì„± ê²€ì¦ (ì¢Œìš° ê³ ì  ë†’ì´ ì°¨ì´ 5% ì´ë‚´)
            left_high = smoothed[left]
            right_high = smoothed[right]
            height_diff = abs(left_high - right_high) / min(left_high, right_high)
            
            return height_diff <= 0.05
            
        except Exception:
            return False
    
    def _verify_bezier_correlation(self, smoothed: np.ndarray, left: int, bottom: int, right: int) -> bool:
        """ë² ì§€ì–´ ê³¡ì„ ê³¼ ìƒê´€ê³„ìˆ˜ ë¹„êµ"""
        try:
            # 7ê°œ ì œì–´ì  ì„ ì •
            quarter1 = left + (bottom - left) // 4
            quarter3 = bottom + (right - bottom) // 4
            mid_left = (left + bottom) // 2
            mid_right = (bottom + right) // 2
            
            control_points = np.array([
                [left, smoothed[left]],
                [quarter1, smoothed[quarter1]],
                [mid_left, smoothed[mid_left]],
                [bottom, smoothed[bottom]],
                [mid_right, smoothed[mid_right]],
                [quarter3, smoothed[quarter3]],
                [right, smoothed[right]]
            ])
            
            # ë² ì§€ì–´ ê³¡ì„  ìƒì„± (ê°„ë‹¨í•œ ê·¼ì‚¬)
            t = np.linspace(0, 1, right - left + 1)
            bezier_curve = self._generate_bezier_curve(control_points, t)
            
            # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            original_segment = smoothed[left:right+1]
            correlation = np.corrcoef(original_segment, bezier_curve)[0, 1]
            
            return correlation >= self.correlation_threshold
            
        except Exception:
            return False
    
    def _generate_bezier_curve(self, control_points: np.ndarray, t: np.ndarray) -> np.ndarray:
        """ë² ì§€ì–´ ê³¡ì„  ìƒì„± (ê°„ë‹¨í•œ ì„ í˜• ë³´ê°„)"""
        # ì‹¤ì œ ë² ì§€ì–´ ê³¡ì„  ëŒ€ì‹  ìŠ¤í”Œë¼ì¸ ë³´ê°„ ì‚¬ìš©
        from scipy.interpolate import interp1d
        
        x_controls = control_points[:, 0]
        y_controls = control_points[:, 1]
        
        # ì •ê·œí™”ëœ të¥¼ ì‹¤ì œ x ì¢Œí‘œë¡œ ë³€í™˜
        x_new = np.linspace(x_controls[0], x_controls[-1], len(t))
        
        # ìŠ¤í”Œë¼ì¸ ë³´ê°„
        f = interp1d(x_controls, y_controls, kind='cubic', fill_value='extrapolate')
        return f(x_new)
    
    def _verify_handle(self, smoothed: np.ndarray, right_peak: int, end: int) -> bool:
        """í•¸ë“¤ ê²€ì¦ (ì»µ ê¹Šì´ì˜ 33% ì´ë‚´ ì¡°ì •)"""
        if end - right_peak < 5:  # ìµœì†Œ 5ì¼ í•¸ë“¤
            return False
        
        handle_segment = smoothed[right_peak:end+1]
        handle_low = np.min(handle_segment)
        right_high = smoothed[right_peak]
        
        # í•¸ë“¤ í•˜ë½í­ ê³„ì‚°
        handle_decline = (right_high - handle_low) / right_high
        
        # ì»µ ê¹Šì´ì˜ 33% ì´ë‚´ì¸ì§€ í™•ì¸
        return handle_decline <= 0.33


def analyze_tickers_from_results(results_dir: str, data_dir: str, output_dir: str = MARKMINERVINI_RESULTS_DIR) -> pd.DataFrame:
    """CSV íŒŒì¼ì—ì„œ í‹°ì»¤ ëª©ë¡ì„ ì½ê³  íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    os.makedirs(output_dir, exist_ok=True)
    results_file = os.path.join(results_dir, "advanced_financial_results.csv")
    
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file}")
    
    logger.info(f"ì¬ë¬´ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘: {results_file}")
    results_df = pd.read_csv(results_file)
    logger.info(f"ì´ {len(results_df)}ê°œ ì¢…ëª©ì— ëŒ€í•œ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
    
    vcp_detector = VCPDetector()
    cup_detector = CupHandleDetector()
    analysis = []
    
    for _, row in results_df.iterrows():
        symbol = row['symbol']
        fin_met_count = row.get('fin_met_count', 0)
        
        file_path = os.path.join(data_dir, f"{symbol}.csv")
        if not os.path.exists(file_path):
            continue
        
        try:
            df = pd.read_csv(file_path)
            date_col = next((c for c in df.columns if c.lower() in ['date', 'ë‚ ì§œ', 'ì¼ì']), None)
            if not date_col:
                continue
            
            df[date_col] = pd.to_datetime(df[date_col], utc=True)
            df.set_index(date_col, inplace=True)
            
            # ì»¬ëŸ¼ëª… ë§¤í•‘
            col_map = {
                'high': ['high', 'High', 'ê³ ê°€'],
                'low': ['low', 'Low', 'ì €ê°€'],
                'close': ['close', 'Close', 'ì¢…ê°€'],
                'volume': ['volume', 'Volume', 'ê±°ë˜ëŸ‰']
            }
            
            found = {}
            for k, names in col_map.items():
                for c in df.columns:
                    if c.lower() in [n.lower() for n in names]:
                        found[k] = c
                        break
            
            if len(found) < 4:
                continue
            
            df = df.rename(columns={v: k for k, v in found.items()})
            
            # íŒ¨í„´ ê°ì§€
            vcp = vcp_detector.detect(df)
            cup = cup_detector.detect(df)
            
            if not vcp and not cup:
                continue
            
            analysis.append({
                'symbol': symbol,
                'fin_met_count': fin_met_count,
                'vcp': vcp,
                'cup_handle': cup,
                'detection_date': datetime.now().strftime('%Y-%m-%d')
            })
            
        except Exception as e:
            logger.error(f"âš ï¸ {symbol} íŒ¨í„´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    if not analysis:
        return pd.DataFrame()
    
    out_df = pd.DataFrame(analysis)
    out_df = out_df.sort_values(['vcp', 'cup_handle'], ascending=False)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d')
    out_file = os.path.join(output_dir, f'academic_pattern_results_{timestamp}.csv')
    out_df.to_csv(out_file, index=False, encoding='utf-8-sig')
    out_df.to_json(out_file.replace('.csv', '.json'), orient='records', indent=2, force_ascii=False)
    
    return out_df


def run_pattern_detection_on_financial_results() -> Optional[pd.DataFrame]:
    """advanced_financial_results.csvì˜ í‹°ì»¤ë“¤ì— ëŒ€í•´ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ ì‹¤í–‰"""
    start_time = datetime.now()
    
    # advanced_financial_results.csv ì½ê¸°
    if not os.path.exists(ADVANCED_FINANCIAL_RESULTS_PATH):
        logger.error(f"âŒ {ADVANCED_FINANCIAL_RESULTS_PATH} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    try:
        financial_df = pd.read_csv(ADVANCED_FINANCIAL_RESULTS_PATH)
        if financial_df.empty:
            logger.warning("âŒ advanced_financial_results.csvê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None
    
    logger.info(f"ğŸ“Š {len(financial_df)} ê°œ ì¢…ëª©ì— ëŒ€í•´ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ğŸ“Š {len(financial_df)} ê°œ ì¢…ëª©ì— ëŒ€í•´ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    vcp_detector = VCPDetector(min_contractions=2)
    cup_detector = CupHandleDetector(correlation_threshold=0.85)
    
    pattern_results = []
    processed_count = 0
    pattern_count = 0
    
    for idx, row in financial_df.iterrows():
        symbol = row['symbol']
        fin_met_count = row.get('fin_met_count', 0)
        processed_count += 1
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if processed_count % max(1, len(financial_df) // 10) == 0:
            progress = processed_count / len(financial_df) * 100
            logger.info(f"ì§„í–‰ ì¤‘: {progress:.1f}% ì™„ë£Œ ({processed_count}/{len(financial_df)})")
        
        try:
            # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì£¼ê°€ ë°ì´í„° ì½ê¸°
            csv_file = f"{symbol}.csv"
            _, stock_data_full, _ = process_stock_data(csv_file, DATA_US_DIR, min_days=60, recent_days=365)
            
            if stock_data_full is None or len(stock_data_full) < 60:
                logger.debug(f"âš ï¸ {symbol}: ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ìµœê·¼ 1ë…„ ë°ì´í„°ë§Œ ì‚¬ìš©
            stock_data = stock_data_full.tail(365).copy()
            
            # date ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            if 'date' in stock_data.columns and not isinstance(stock_data.index, pd.DatetimeIndex):
                stock_data = stock_data.set_index('date')
            
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            
            # ì»¬ëŸ¼ëª… ë§¤í•‘
            col_mapping = {}
            for req_col in required_cols:
                for col in stock_data.columns:
                    if req_col.lower() == col.lower() or req_col.lower() in col.lower():
                        col_mapping[col] = req_col
                        break
            
            if len(col_mapping) < len(required_cols):
                missing_cols = set(required_cols) - set(col_mapping.values())
                logger.debug(f"âš ï¸ {symbol}: í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ëˆ„ë½: {missing_cols}")
                continue
            
            # ì»¬ëŸ¼ëª… ë³€ê²½
            stock_data = stock_data.rename(columns=col_mapping)
            
            # í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€
            vcp_detected = vcp_detector.detect(stock_data)
            cup_detected = cup_detector.detect(stock_data)
            
            # í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ ê²°ê³¼ì— ì¶”ê°€
            if vcp_detected or cup_detected:
                pattern_count += 1
                pattern_results.append({
                    'symbol': symbol,
                    'fin_met_count': fin_met_count,
                    'rs_score': row.get('rs_score', None),
                    'rs_percentile': row.get('rs_percentile', None),
                    'fin_percentile': row.get('fin_percentile', None),
                    'total_percentile': row.get('total_percentile', None),
                    'vcp_pattern': vcp_detected,
                    'cup_handle_pattern': cup_detected,
                    'has_error': row.get('has_error', False),
                    'detection_date': datetime.now().strftime('%Y-%m-%d'),
                    'detection_method': 'Academic (Kernel Regression + Bezier)'
                })
                logger.info(f"âœ… {symbol}: VCP={vcp_detected}, Cup&Handle={cup_detected}")
                print(f"âœ… {symbol}: VCP={vcp_detected}, Cup&Handle={cup_detected}")
            
        except Exception as e:
            logger.error(f"âš ï¸ {symbol} íŒ¨í„´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    elapsed_time = datetime.now() - start_time
    
    # ê²°ê³¼ ì €ì¥
    output_dir = os.path.dirname(ADVANCED_FINANCIAL_RESULTS_PATH)
    timestamp = datetime.now().strftime('%Y%m%d')
    
    if pattern_results:
        results_df = pd.DataFrame(pattern_results)
        
        # ì •ë ¬
        results_df['pattern_score'] = results_df['vcp_pattern'].astype(int) + results_df['cup_handle_pattern'].astype(int)
        results_df = results_df.sort_values(['pattern_score', 'total_percentile'], ascending=[False, False])
        results_df = results_df.drop('pattern_score', axis=1)
        
        # íŒŒì¼ ì €ì¥
        csv_path = os.path.join(output_dir, f'academic_pattern_results_{timestamp}.csv')
        json_path = os.path.join(output_dir, f'academic_pattern_results_{timestamp}.json')
        latest_csv_path = os.path.join(output_dir, 'academic_pattern_results.csv')
        latest_json_path = os.path.join(output_dir, 'academic_pattern_results.json')
        
        try:
            results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            results_df.to_json(json_path, orient='records', indent=2, force_ascii=False)
            results_df.to_csv(latest_csv_path, index=False, encoding='utf-8-sig')
            results_df.to_json(latest_json_path, orient='records', indent=2, force_ascii=False)
            
            logger.info(f"\nğŸ¯ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ ì™„ë£Œ: {len(results_df)}ê°œ ì¢…ëª©ì´ íŒ¨í„´ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
            print(f"\nğŸ¯ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ ì™„ë£Œ: {len(results_df)}ê°œ ì¢…ëª©ì´ íŒ¨í„´ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {csv_path}")
            print(f"ğŸ“ ìµœì‹  ê²°ê³¼: {latest_csv_path}")
            
            # ìƒìœ„ 10ê°œ ê²°ê³¼ ì¶œë ¥
            print("\nğŸ† ìƒìœ„ 10ê°œ íŒ¨í„´ ê°ì§€ ê²°ê³¼:")
            top_10 = results_df.head(10)
            print(top_10[['symbol', 'fin_met_count', 'vcp_pattern', 'cup_handle_pattern', 'total_percentile', 'detection_method']])
            
            # ì‹¤í–‰ í†µê³„ ì¶œë ¥
            print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed_time}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜: {processed_count}")
            print(f"âœ… íŒ¨í„´ ê°ì§€ëœ ì¢…ëª© ìˆ˜: {pattern_count}")
            print(f"ğŸ“ˆ íŒ¨í„´ ê°ì§€ ë¹„ìœ¨: {pattern_count/processed_count*100:.2f}%")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return results_df
    else:
        logger.warning("âŒ íŒ¨í„´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("âŒ íŒ¨í„´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„±
        empty_df = pd.DataFrame(columns=[
            'symbol', 'fin_met_count', 'rs_score', 'rs_percentile', 
            'fin_percentile', 'total_percentile', 'vcp_pattern', 
            'cup_handle_pattern', 'has_error', 'detection_date', 'detection_method'
        ])
        
        csv_path = os.path.join(output_dir, f'academic_pattern_results_{timestamp}.csv')
        latest_csv_path = os.path.join(output_dir, 'academic_pattern_results.csv')
        
        try:
            empty_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            empty_df.to_csv(latest_csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„±: {csv_path}")
            print(f"\nâ±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed_time}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜: {processed_count}")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return empty_df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ VCP ë° Cup & Handle íŒ¨í„´ ê°ì§€ ì‹œì‘")
        print("ğŸ”¬ í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ íŒ¨í„´ ê°ì§€ ì‹œìŠ¤í…œ ì‹œì‘")
        print("ğŸ“š ì ìš© ë…¼ë¬¸: Lo, Mamaysky & Wang (2000), Suh, Li & Gao (2008)")
        print("ğŸ”§ ë°©ë²•ë¡ : ì»¤ë„ íšŒê·€ ìŠ¤ë¬´ë”© + ë² ì§€ì–´ ê³¡ì„  ìƒê´€ê³„ìˆ˜ ë¶„ì„\n")
        
        result = run_pattern_detection_on_financial_results()
        
        if result is not None and not result.empty:
            logger.info("âœ… íŒ¨í„´ ê°ì§€ ì™„ë£Œ")
            return 0
        else:
            logger.warning("âš ï¸ ê°ì§€ëœ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤")
            return 0
            
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)