#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìŠ¤í¬ë¦¬ë„ˆ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
ìƒˆë¡œìš´ í‹°ì»¤ ì¶”ì , JSON/CSV ì €ì¥ ë“±ì˜ ê³µí†µ ê¸°ëŠ¥ ì œê³µ
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
from utils import ensure_dir
import glob
import re


def convert_numpy_types(obj):
    """numpy íƒ€ì…ê³¼ pandas Timestampë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ Python native íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, pd.Timestamp):
        return obj.strftime('%Y-%m-%d %H:%M:%S')
    elif hasattr(obj, 'timestamp'):  # pandas Timestamp ê°ì²´ ì²˜ë¦¬
        return obj.strftime('%Y-%m-%d %H:%M:%S')
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    return obj


def find_latest_file(directory: str, prefix: str, extension: str) -> Optional[str]:
    """
    ë””ë ‰í† ë¦¬ì—ì„œ íŠ¹ì • ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ê°€ì¥ ìµœì‹  íŒŒì¼ì„ ì°¾ê¸°
    ì‹œê°„ ì •ë³´ê°€ ì œê±°ëœ íŒŒì¼ëª…ë„ ì²˜ë¦¬
    
    Args:
        directory: ê²€ìƒ‰í•  ë””ë ‰í† ë¦¬
        prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
        extension: íŒŒì¼ í™•ì¥ì (ì  ì œì™¸)
    
    Returns:
        ìµœì‹  íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ ë˜ëŠ” None
    """
    if not os.path.exists(directory):
        return None
    
    matching_files = []
    for file in os.listdir(directory):
        # ì •í™•í•œ ì ‘ë‘ì‚¬ ë§¤ì¹­: ì ‘ë‘ì‚¬ë¡œ ì‹œì‘í•˜ê³ , ê·¸ ë‹¤ìŒì´ '_', '.', ë˜ëŠ” íŒŒì¼ ë
        if (file.startswith(prefix) and file.endswith(f'.{extension}') and
            (len(file) == len(prefix) + len(extension) + 1 or  # prefix.ext
             file[len(prefix)] in ['_', '.'])):
            file_path = os.path.join(directory, file)
            matching_files.append((file_path, os.path.getmtime(file_path)))
    
    if not matching_files:
        return None
    
    # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœì‹  íŒŒì¼ ë°˜í™˜
    latest_file = max(matching_files, key=lambda x: x[1])[0]
    print(f"[Utils] find_latest_file: {os.path.basename(latest_file)} (ì´ {len(matching_files)}ê°œ ì¤‘)")
    return latest_file


def read_csv_flexible(file_path: str, required_columns: List[str] = None) -> Optional[pd.DataFrame]:
    """
    CSV íŒŒì¼ì„ ìœ ì—°í•˜ê²Œ ì½ê¸° - ì»¬ëŸ¼ëª… ë³€í™”ì— ëŒ€ì‘
    
    Args:
        file_path: CSV íŒŒì¼ ê²½ë¡œ
        required_columns: í•„ìˆ˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ëª¨ë“  ì»¬ëŸ¼ í—ˆìš©)
    
    Returns:
        DataFrame ë˜ëŠ” None (ì½ê¸° ì‹¤íŒ¨ ì‹œ)
    """
    if not os.path.exists(file_path):
        return None
    
    try:
        df = pd.read_csv(file_path)
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (ì†Œë¬¸ì, ê³µë°± ì œê±°)
        df.columns = [col.lower().strip() for col in df.columns]
        
        # VIX ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬: vix_close -> close ë§¤í•‘
        if 'vix_close' in df.columns and 'close' not in df.columns:
            df['close'] = df['vix_close']
            print(f"ğŸ“Š VIX ë°ì´í„° ë§¤í•‘: vix_close -> close")
        
        # ê¸°íƒ€ ì»¬ëŸ¼ ë§¤í•‘ ì²˜ë¦¬
        column_mappings = {
            'vix_high': 'high',
            'vix_low': 'low',
            'vix_volume': 'volume'
        }
        
        for old_col, new_col in column_mappings.items():
            if old_col in df.columns and new_col not in df.columns:
                df[new_col] = df[old_col]
                print(f"ğŸ“Š ì»¬ëŸ¼ ë§¤í•‘: {old_col} -> {new_col}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if required_columns:
            missing_cols = [col for col in required_columns if col not in df.columns]
            if missing_cols:
                print(f"âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols} in {file_path}")
                return None
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        date_columns = ['date', 'processing_date', 'ì²­ì‚°ì¼ì‹œ', 'added_date']
        for col in date_columns:
            if col in df.columns and not df[col].empty and not df[col].isna().all():
                try:
                    # ì‹œê°„ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš° ë‚ ì§œë§Œ ì¶”ì¶œ
                    if df[col].dtype == 'object':
                        df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)
                        if col in ['processing_date', 'ì²­ì‚°ì¼ì‹œ']:  # ì‹œê°„ ì •ë³´ ì œê±°ê°€ í•„ìš”í•œ ì»¬ëŸ¼
                            df[col] = df[col].dt.strftime('%Y-%m-%d')
                except Exception as e:
                    print(f"âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ '{col}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return df
        
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path}): {e}")
        return None


def save_screening_results(results: List[Dict[str, Any]], 
                          output_dir: str, 
                          filename_prefix: str,
                          include_timestamp: bool = False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½
                          incremental_update: bool = True) -> Dict[str, str]:
    """
    ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼ë¥¼ JSONê³¼ CSV í˜•íƒœë¡œ ì €ì¥ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)
    
    Args:
        results: ì €ì¥í•  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        filename_prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
        include_timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì—¬ë¶€ (ë‚ ì§œë§Œ í¬í•¨)
        incremental_update: ì¦ë¶„ ì—…ë°ì´íŠ¸ ì—¬ë¶€
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤ (csv_path, json_path)
    """
    ensure_dir(output_dir)
    
    # íŒŒì¼ëª… ìƒì„± (ì‹œê°„ ì •ë³´ ì—†ì´)
    if include_timestamp:
        timestamp = datetime.now().strftime('%Y%m%d')
        base_filename = f"{filename_prefix}_{timestamp}"
    else:
        base_filename = filename_prefix
    
    csv_path = os.path.join(output_dir, f"{base_filename}.csv")
    json_path = os.path.join(output_dir, f"{base_filename}.json")
    
    # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œ ê¸°ì¡´ íŒŒì¼ ì°¾ê¸°
    if incremental_update and not os.path.exists(csv_path):
        existing_csv = find_latest_file(output_dir, filename_prefix, 'csv')
        if existing_csv:
            csv_path = existing_csv
        existing_json = find_latest_file(output_dir, filename_prefix, 'json')
        if existing_json:
            json_path = existing_json
    
    if len(results) > 0:
        new_df = pd.DataFrame(results)
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if incremental_update and os.path.exists(csv_path):
            try:
                existing_df = read_csv_flexible(csv_path)
                if existing_df is None:
                    raise Exception("íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")
                
                # ê¸°ë³¸ í‚¤ ì»¬ëŸ¼ í™•ì¸ (symbol ë˜ëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼)
                key_col = 'symbol' if 'symbol' in new_df.columns else new_df.columns[0]
                
                if key_col in existing_df.columns:
                    # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìƒˆ ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” í•­ëª© ì œê±°
                    existing_df = existing_df[~existing_df[key_col].isin(new_df[key_col])]
                    
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•© (ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬)
                    if existing_df.empty and not new_df.empty:
                        combined_df = new_df.copy()
                    elif not existing_df.empty and new_df.empty:
                        combined_df = existing_df.copy()
                    elif not existing_df.empty and not new_df.empty:
                        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    else:
                        combined_df = pd.DataFrame()
                    
                    # ì •ë ¬ ìœ ì§€ (ê¸°ì¡´ íŒŒì¼ì˜ ì •ë ¬ ë°©ì‹ í™•ì¸)
                    if len(existing_df) > 1:
                        # ì²« ë²ˆì§¸ ì •ë ¬ ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬ ë°©í–¥ í™•ì¸
                        sort_col = None
                        for col in combined_df.columns:
                            if combined_df[col].dtype in ['int64', 'float64', 'datetime64[ns]'] or col == key_col:
                                sort_col = col
                                break
                        
                        if sort_col:
                            # ê¸°ì¡´ ë°ì´í„°ì˜ ì •ë ¬ ë°©í–¥ í™•ì¸
                            if len(existing_df) >= 2:
                                is_ascending = existing_df[sort_col].iloc[0] <= existing_df[sort_col].iloc[1]
                                combined_df = combined_df.sort_values(sort_col, ascending=is_ascending)
                    
                    final_df = combined_df
                    print(f"ğŸ”„ ì¦ë¶„ ì—…ë°ì´íŠ¸: ê¸°ì¡´ {len(existing_df)}ê°œ + ì‹ ê·œ {len(new_df)}ê°œ = ì´ {len(final_df)}ê°œ")
                else:
                    final_df = new_df
                    print(f"âš ï¸ í‚¤ ì»¬ëŸ¼ '{key_col}' ë¶ˆì¼ì¹˜, ì „ì²´ êµì²´: {len(new_df)}ê°œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({e}), ì „ì²´ êµì²´: {len(new_df)}ê°œ")
                final_df = new_df
        else:
            final_df = new_df
            print(f"âœ… ì‹ ê·œ ì €ì¥: {len(new_df)}ê°œ ì¢…ëª©")
        
        # CSV ì €ì¥
        final_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # JSON ì €ì¥ (numpy íƒ€ì… ë³€í™˜)
        if incremental_update and os.path.exists(json_path):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_json = json.load(f)
                
                # JSONë„ ë™ì¼í•˜ê²Œ ì¦ë¶„ ì—…ë°ì´íŠ¸
                key_col = 'symbol' if 'symbol' in results[0] else list(results[0].keys())[0]
                existing_keys = {item.get(key_col) for item in existing_json if key_col in item}
                new_items = [item for item in results if item.get(key_col) not in existing_keys]
                
                # ê¸°ì¡´ í•­ëª©ì—ì„œ ì—…ë°ì´íŠ¸ëœ í•­ëª© ì œê±°
                updated_existing = [item for item in existing_json 
                                  if item.get(key_col) not in {r.get(key_col) for r in results}]
                
                combined_json = updated_existing + convert_numpy_types(results)
            except Exception:
                combined_json = convert_numpy_types(results)
        else:
            combined_json = convert_numpy_types(results)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(combined_json, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ“„ CSV: {csv_path}")
        print(f"   ğŸ“„ JSON: {json_path}")
    else:
        # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„±
        pd.DataFrame().to_csv(csv_path, index=False)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump([], f)
        
        print(f"âš ï¸  ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„±: {csv_path}")
    
    return {
        'csv_path': csv_path,
        'json_path': json_path
    }


def track_new_tickers(current_results: List[Dict[str, Any]], 
                     tracker_file: str,
                     symbol_key: str = 'symbol',
                     retention_days: int = 14) -> List[Dict[str, Any]]:
    """
    ìƒˆë¡œìš´ í‹°ì»¤ë¥¼ ì¶”ì í•˜ê³  ê´€ë¦¬
    
    Args:
        current_results: í˜„ì¬ ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼
        tracker_file: ì¶”ì  íŒŒì¼ ê²½ë¡œ (CSV)
        symbol_key: ì‹¬ë³¼ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤
        retention_days: ë°ì´í„° ë³´ì¡´ ê¸°ê°„ (ì¼)
    
    Returns:
        ìƒˆë¡œ ë°œê²¬ëœ í‹°ì»¤ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    current_symbols = {item[symbol_key] for item in current_results if symbol_key in item}
    
    # ê¸°ì¡´ ì¶”ì  ë°ì´í„° ë¡œë“œ
    if os.path.exists(tracker_file):
        existing_df = read_csv_flexible(tracker_file, [symbol_key])
        if existing_df is not None:
            existing_symbols = set(existing_df[symbol_key].tolist()) if symbol_key in existing_df.columns else set()
        else:
            print(f"âš ï¸  ì¶”ì  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {tracker_file}")
            existing_symbols = set()
            existing_df = pd.DataFrame()
    else:
        existing_symbols = set()
        existing_df = pd.DataFrame()
    
    # ìƒˆë¡œìš´ í‹°ì»¤ ì‹ë³„
    new_symbols = current_symbols - existing_symbols
    
    if new_symbols:
        print(f"ğŸ†• ìƒˆë¡œìš´ í‹°ì»¤ ë°œê²¬: {len(new_symbols)}ê°œ")
        
        # ìƒˆë¡œìš´ í‹°ì»¤ ë°ì´í„° ìƒì„±
        new_ticker_data = []
        for symbol in new_symbols:
            # í˜„ì¬ ê²°ê³¼ì—ì„œ í•´ë‹¹ ì‹¬ë³¼ì˜ ë°ì´í„° ì°¾ê¸°
            symbol_data = next((item for item in current_results if item.get(symbol_key) == symbol), {})
            
            new_ticker_data.append({
                symbol_key: symbol,
                'added_date': datetime.now().strftime('%Y-%m-%d'),
                'added_timestamp': datetime.now().timestamp(),
                **{k: v for k, v in symbol_data.items() if k != symbol_key}
            })
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
        if not existing_df.empty:
            # ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (retention_days ì´ìƒ)
            cutoff_date = datetime.now().timestamp() - (retention_days * 24 * 3600)
            if 'added_timestamp' in existing_df.columns:
                existing_df = existing_df[existing_df['added_timestamp'] >= cutoff_date]
        
        # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
        new_df = pd.DataFrame(new_ticker_data)
        if not existing_df.empty:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        # ì¶”ì  íŒŒì¼ ì €ì¥
        ensure_dir(os.path.dirname(tracker_file))
        combined_df.to_csv(tracker_file, index=False, encoding='utf-8-sig')
        
        # JSON íŒŒì¼ë„ ì €ì¥
        json_file = tracker_file.replace('.csv', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            # pandas Timestamp ë“±ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            data_dict = convert_numpy_types(combined_df.to_dict('records'))
            json.dump(data_dict, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ“„ ì¶”ì  íŒŒì¼ ì—…ë°ì´íŠ¸: {tracker_file}")
        return new_ticker_data
    else:
        print("ğŸ” ìƒˆë¡œìš´ í‹°ì»¤ ì—†ìŒ")
        return []


def create_screener_summary(screener_name: str, 
                          total_candidates: int,
                          new_tickers: int,
                          results_paths: Dict[str, str]) -> Dict[str, Any]:
    """
    ìŠ¤í¬ë¦¬ë„ˆ ì‹¤í–‰ ìš”ì•½ ì •ë³´ ìƒì„±
    
    Args:
        screener_name: ìŠ¤í¬ë¦¬ë„ˆ ì´ë¦„
        total_candidates: ì´ í›„ë³´ ì¢…ëª© ìˆ˜
        new_tickers: ìƒˆë¡œìš´ í‹°ì»¤ ìˆ˜
        results_paths: ê²°ê³¼ íŒŒì¼ ê²½ë¡œë“¤
    
    Returns:
        ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    return {
        'screener_name': screener_name,
        'execution_time': datetime.now().isoformat(),
        'total_candidates': total_candidates,
        'new_tickers_found': new_tickers,
        'results_files': results_paths,
        'status': 'completed'
    }